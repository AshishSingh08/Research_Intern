{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd \n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import KFold\n","from sklearn import preprocessing\n","from sklearn.metrics import confusion_matrix\n","from sklearn.feature_selection import VarianceThreshold\n","from sklearn.decomposition import PCA\n","from tensorflow.keras import layers,regularizers,Sequential,Model,backend,callbacks,optimizers,metrics,losses\n","import tensorflow as tf\n","import sys\n","import json\n","sys.path.append('../input/iterative-stratification/iterative-stratification-master')\n","from iterstrat.ml_stratifiers import MultilabelStratifiedKFold"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"outputs":[],"source":["\n","\n","train_features = pd.read_csv('/kaggle/input/lish-moa/train_features.csv')\n","non_ctl_idx = train_features.loc[train_features['cp_type']!='ctl_vehicle'].index.to_list()\n","train_features = train_features.drop(['sig_id','cp_type','cp_dose','cp_time'],axis=1)\n","train_targets_scored = pd.read_csv('/kaggle/input/lish-moa/train_targets_scored.csv')\n","train_targets_scored = train_targets_scored.drop('sig_id',axis=1)\n","labels_train = train_targets_scored.values\n","\n","\n","train_features = train_features.iloc[non_ctl_idx]\n","labels_train = labels_train[non_ctl_idx]\n","\n","\n","\n","test_features = pd.read_csv('/kaggle/input/lish-moa/test_features.csv')\n","test_features = test_features.drop(['sig_id','cp_dose','cp_time'],axis=1)\n","\n","\n","\n","json_file_path = '../input/t-test-pca-rfe-logistic-regression/main_predictors.json'\n","\n","with open(json_file_path, 'r') as j:\n","    predictors = json.loads(j.read())\n","    predictors = predictors['start_predictors']"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","\n","cs = train_features.columns.str.contains('c-')\n","gs = train_features.columns.str.contains('g-')\n","\n","def preprocessor(train,test):\n","    \n","    \n","    \n","    n_gs = 2 \n","    n_cs = 100 \n","    \n","    pca_cs = PCA(n_components = n_cs)\n","    pca_gs = PCA(n_components = n_gs)\n","\n","    train_pca_gs = pca_gs.fit_transform(train[:,gs])\n","    train_pca_cs = pca_cs.fit_transform(train[:,cs])\n","    test_pca_gs = pca_gs.transform(test[:,gs])\n","    test_pca_cs = pca_cs.transform(test[:,cs])\n","    \n","    \n","    train_c_mean = train[:,cs].mean(axis=1)\n","    test_c_mean = test[:,cs].mean(axis=1)\n","    train_g_mean = train[:,gs].mean(axis=1)\n","    test_g_mean = test[:,gs].mean(axis=1)\n","    \n","   \n","    \n","    train = np.concatenate((train,train_pca_gs,train_pca_cs,train_c_mean[:,np.newaxis]\n","                            ,train_g_mean[:,np.newaxis]),axis=1)\n","    test = np.concatenate((test,test_pca_gs,test_pca_cs,test_c_mean[:,np.newaxis],\n","                           test_g_mean[:,np.newaxis]),axis=1)\n","    \n"," \n","    scaler = preprocessing.StandardScaler()\n","\n","    train = scaler.fit_transform(train)\n","\n","    \n","    test = scaler.transform(test)\n","    \n","    return train, test\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["n_labels = train_targets_scored.shape[1]\n","n_train = train_features.shape[0]\n","n_test = test_features.shape[0]\n","\n","\n","\n","\n","p_min = 0.0005\n","p_max = 0.9995\n","\n","\n","\n","def logloss(y_true, y_pred):\n","    y_pred = tf.clip_by_value(y_pred,p_min,p_max)\n","    return -backend.mean(y_true*backend.log(y_pred) + (1-y_true)*backend.log(1-y_pred))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def build_model(n_features, n_features_2, n_labels, label_smoothing = 0.0005):    \n","    input_1 = layers.Input(shape = (n_features,), name = 'Input1')\n","    input_2 = layers.Input(shape = (n_features_2,), name = 'Input2')\n","\n","    head_1 = Sequential([\n","        layers.BatchNormalization(),\n","        layers.Dropout(0.2),\n","        layers.Dense(512, activation=\"elu\"), \n","        layers.BatchNormalization(),\n","        layers.Dense(256, activation = \"elu\")\n","        ],name='Head1') \n","\n","    input_3 = head_1(input_1)\n","    input_3_concat = layers.Concatenate()([input_2, input_3])\n","\n","    head_2 = Sequential([\n","        layers.BatchNormalization(),\n","        layers.Dropout(0.3),\n","        layers.Dense(512, \"relu\"),\n","        layers.BatchNormalization(),\n","        layers.Dense(512, \"elu\"),\n","        layers.BatchNormalization(),\n","        layers.Dense(256, \"relu\"),\n","        layers.BatchNormalization(),\n","        layers.Dense(256, \"elu\")\n","        ],name='Head2')\n","\n","    input_4 = head_2(input_3_concat)\n","    input_4_avg = layers.Average()([input_3, input_4]) \n","\n","    head_3 = Sequential([\n","        layers.BatchNormalization(),\n","        layers.Dense(256, kernel_initializer='lecun_normal', activation='selu'),\n","        layers.BatchNormalization(),\n","        layers.Dense(n_labels, kernel_initializer='lecun_normal', activation='selu'),\n","        layers.BatchNormalization(),\n","        layers.Dense(n_labels, activation=\"sigmoid\")\n","        ],name='Head3')\n","\n","    output = head_3(input_4_avg)\n","\n","\n","    model = Model(inputs = [input_1, input_2], outputs = output)\n","    model.compile(optimizer='adam', loss=losses.BinaryCrossentropy(label_smoothing=label_smoothing), metrics=logloss)\n","    \n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","n_seeds = 7\n","np.random.seed(1)\n","seeds = np.random.randint(0,100,size=n_seeds)\n","\n","\n","n_folds = 10\n","y_pred = np.zeros((n_test,n_labels))\n","oof = tf.constant(0.0)\n","hists = []\n","for seed in seeds:\n","    fold = 0\n","    kf = KFold(n_splits=n_folds,shuffle=True,random_state=seed)\n","    for train, test in kf.split(train_features):\n","        X_train, X_test = preprocessor(train_features.iloc[train].values,\n","                                       train_features.iloc[test].values)\n","        _,data_test = preprocessor(train_features.iloc[train].values,\n","                                   test_features.drop('cp_type',axis=1).values)\n","        X_train_2 = train_features.iloc[train][predictors].values\n","        X_test_2 = train_features.iloc[test][predictors].values\n","        data_test_2 = test_features[predictors].values\n","        y_train = labels_train[train]\n","        y_test = labels_train[test]\n","        n_features = X_train.shape[1]\n","        n_features_2 = X_train_2.shape[1]\n","\n","        model = build_model(n_features, n_features_2, n_labels)\n","        \n","        reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_logloss', factor=0.1, patience=2, mode='min', min_lr=1E-5)\n","        early_stopping = callbacks.EarlyStopping(monitor='val_logloss', min_delta=1E-5, patience=10, mode='min',restore_best_weights=True)\n","\n","        hist = model.fit([X_train,X_train_2],y_train, batch_size=128, epochs=192,verbose=0,validation_data = ([X_test,X_test_2],y_test),\n","                         callbacks=[reduce_lr, early_stopping])\n","        hists.append(hist)\n","        \n","       \n","        model.save('TwoHeads_seed_'+str(seed)+'_fold_'+str(fold))\n","\n","       \n","        y_val = model.predict([X_test,X_test_2])\n","        oof += logloss(tf.constant(y_test,dtype=tf.float32),tf.constant(y_val,dtype=tf.float32))/(n_folds*n_seeds)\n","\n","        \n","        y_pred += model.predict([data_test,data_test_2])/(n_folds*n_seeds)\n","\n","        fold += 1"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","\n","tf.keras.utils.plot_model(model,show_shapes=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","\n","tf.print('OOF score is ',oof)\n","\n","plt.figure(figsize=(12,8))\n","\n","hist_trains = []\n","hist_lens = []\n","for i in range(n_folds*n_seeds):\n","    hist_train = (hists[i]).history['logloss']\n","    hist_trains.append(hist_train)\n","    hist_lens.append(len(hist_train))\n","hist_train = []\n","for i in range(min(hist_lens)):\n","    hist_train.append(np.mean([hist_trains[j][i] for j in range(n_folds*n_seeds)]))\n","\n","plt.plot(hist_train)\n","\n","hist_vals = []\n","hist_lens = []\n","for i in range(n_folds*n_seeds):\n","    hist_val = (hists[i]).history['val_logloss']\n","    hist_vals.append(hist_val)\n","    hist_lens.append(len(hist_val))\n","hist_val = []\n","for i in range(min(hist_lens)):\n","    hist_val.append(np.mean([hist_vals[j][i] for j in range(n_folds*n_seeds)]))\n","\n","plt.plot(hist_val)\n","\n","plt.yscale('log')\n","plt.yticks(ticks=[1,1E-1,1E-2])\n","plt.xlabel('Epochs')\n","plt.ylabel('Average Logloss')\n","plt.legend(['Training','Validation'])"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"databundleVersionId":1651354,"sourceId":19988,"sourceType":"competition"},{"datasetId":857477,"sourceId":1462276,"sourceType":"datasetVersion"},{"sourceId":42716634,"sourceType":"kernelVersion"}],"dockerImageVersionId":30009,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
