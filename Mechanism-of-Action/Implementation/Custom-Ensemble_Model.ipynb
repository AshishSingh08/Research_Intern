{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2020-11-11T02:55:48.244Z","iopub.status.busy":"2020-11-11T02:55:48.243231Z","iopub.status.idle":"2020-11-11T02:55:48.245617Z","shell.execute_reply":"2020-11-11T02:55:48.246198Z"},"id":"qVsDeXAFZnL9","papermill":{"duration":0.059814,"end_time":"2020-11-11T02:55:48.246319","exception":false,"start_time":"2020-11-11T02:55:48.186505","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["LABEL_SMOOTHING=0.001\n","\n","SEEDS = [120,2524]\n","SPLITS = 7\n","BATCH_SIZE = 64\n","EPOCHS = 60\n","\n","VARIANCE_THRESHOLD = True\n","\n","ADD_NON_TRAIN = True\n","ADD_SEEDS = SEEDS\n","ADD_SPLITS = SPLITS\n","ADD_BATCH_SIZE = BATCH_SIZE\n","\n","ADD_EPOCHS = EPOCHS\n","\n","RUN_SNN = True\n","SNN_SEEDS = SEEDS + [42]\n","SNN_SPLITS = SPLITS\n","SNN_BATCH_SIZE = BATCH_SIZE\n","SNN_EPOCHS = EPOCHS\n","\n","RUN_NN = True\n","NN_SEEDS = SEEDS \n","NN_SPLITS = SPLITS\n","NN_BATCH_SIZE = BATCH_SIZE\n","NN_EPOCHS = EPOCHS\n","\n","RUN_NN2 = True\n","NN2_SEEDS = SEEDS\n","NN2_SPLITS = SPLITS\n","NN2_BATCH_SIZE = BATCH_SIZE\n","NN2_EPOCHS = EPOCHS\n","\n","RUN_RNN = True\n","RNN_SEEDS = SEEDS\n","RNN_SPLITS = SPLITS\n","RNN_BATCH_SIZE = 128\n","RNN_EPOCHS = EPOCHS\n","\n","RUN_TABNET = True\n","TAB_SEEDS = SEEDS\n","TAB_SPLITS = SPLITS\n","TAB_BATCH_SIZE = BATCH_SIZE\n","TAB_EPOCHS = EPOCHS\n","\n","RUN_STACKING = True\n","STK_SEEDS = SEEDS + [42]\n","STK_SPLITS = SPLITS \n","STK_EPOCHS = EPOCHS\n","STK_BATCH_SIZE = BATCH_SIZE\n","\n","RE_RUN_NN = True\n","RE_NN_SEEDS = SEEDS"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.execute_input":"2020-11-11T02:55:48.343481Z","iopub.status.busy":"2020-11-11T02:55:48.34286Z","iopub.status.idle":"2020-11-11T02:55:53.300319Z","shell.execute_reply":"2020-11-11T02:55:53.300968Z"},"id":"2KAILSPTZnMF","papermill":{"duration":5.009089,"end_time":"2020-11-11T02:55:53.301142","exception":false,"start_time":"2020-11-11T02:55:48.292053","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import random\n","import os\n","\n","import tensorflow as tf\n","import tensorflow.keras.layers as L\n","import tensorflow.keras.models as M\n","import tensorflow.keras.backend as K\n","import tensorflow_addons as tfa\n","from tensorflow_addons.layers import WeightNormalization\n","from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2020-11-11T02:55:53.403625Z","iopub.status.busy":"2020-11-11T02:55:53.403024Z","iopub.status.idle":"2020-11-11T02:55:55.506496Z","shell.execute_reply":"2020-11-11T02:55:55.507108Z"},"id":"n_XsWsypZnMM","papermill":{"duration":2.157691,"end_time":"2020-11-11T02:55:55.507319","exception":false,"start_time":"2020-11-11T02:55:53.349628","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["import sys\n","sys.path.append('../input/interactivestratification/iterative-stratification-master')\n","sys.path.append('../input/pytorchtabnet/tabnet-develop')\n","from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n","from pytorch_tabnet import tab_network"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2020-11-11T02:55:55.614987Z","iopub.status.busy":"2020-11-11T02:55:55.614364Z","iopub.status.idle":"2020-11-11T02:55:55.618474Z","shell.execute_reply":"2020-11-11T02:55:55.617782Z"},"id":"f3vwfy-7ZnMT","papermill":{"duration":0.063707,"end_time":"2020-11-11T02:55:55.618577","exception":false,"start_time":"2020-11-11T02:55:55.55487","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["from sklearn.metrics import log_loss\n","import torch\n","\n","def seed_everything(seed=42):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    tf.random.set_seed(seed)\n","    torch.manual_seed(seed)  \n","    session_conf = tf.compat.v1.ConfigProto(\n","        intra_op_parallelism_threads=1,\n","        inter_op_parallelism_threads=1\n","    )\n","    sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n","    tf.compat.v1.keras.backend.set_session(sess)\n","\n","def swish(x):\n","    return x * K.sigmoid(x)\n","\n","def show_metrics(valid_preds, show_each=True):\n","    metrics = []\n","    for _target in valid_preds.columns:\n","      logloss = log_loss(train_targets_scored.iloc[:,1:].loc[:, _target], valid_preds.loc[:, _target])\n","      metrics.append(logloss)\n","      if show_each:\n","        print(f'column: {_target}, log loss: {logloss}')\n","    print(f'OOF Metric: {np.mean(metrics)}')\n","\n","    return metrics"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2020-11-11T02:55:55.721441Z","iopub.status.busy":"2020-11-11T02:55:55.720287Z","iopub.status.idle":"2020-11-11T02:56:02.524142Z","shell.execute_reply":"2020-11-11T02:56:02.523088Z"},"id":"k4DlmDRYZnMa","papermill":{"duration":6.858108,"end_time":"2020-11-11T02:56:02.52429","exception":false,"start_time":"2020-11-11T02:55:55.666182","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["train_features = pd.read_csv(f'../input/lish-moa/train_features.csv')\n","train_targets_scored = pd.read_csv(f'../input/lish-moa/train_targets_scored.csv')\n","train_targets_nonscored = pd.read_csv(f'../input/lish-moa/train_targets_nonscored.csv')\n","\n","test_features = pd.read_csv(f'../input/lish-moa/test_features.csv')\n","sample_submission = pd.read_csv(f'../input/lish-moa/sample_submission.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2020-11-11T02:56:02.625748Z","iopub.status.busy":"2020-11-11T02:56:02.623959Z","iopub.status.idle":"2020-11-11T02:56:02.62643Z","shell.execute_reply":"2020-11-11T02:56:02.626898Z"},"id":"xSIbfGleZnMf","papermill":{"duration":0.05551,"end_time":"2020-11-11T02:56:02.627034","exception":false,"start_time":"2020-11-11T02:56:02.571524","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["GENES = [col for col in train_features.columns if col.startswith('g-')]\n","CELLS = [col for col in train_features.columns if col.startswith('c-')]"]},{"cell_type":"markdown","metadata":{},"source":["## Feature Engineering \n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2020-11-11T02:56:02.726537Z","iopub.status.busy":"2020-11-11T02:56:02.725317Z","iopub.status.idle":"2020-11-11T02:56:02.814281Z","shell.execute_reply":"2020-11-11T02:56:02.81373Z"},"id":"n0KWRxEIZnMj","papermill":{"duration":0.14039,"end_time":"2020-11-11T02:56:02.814418","exception":false,"start_time":"2020-11-11T02:56:02.674028","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["_X_train = train_features.copy()\n","_X_test = test_features.copy()\n","_y_train = train_targets_scored.copy()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2020-11-11T02:56:02.913377Z","iopub.status.busy":"2020-11-11T02:56:02.912643Z","iopub.status.idle":"2020-11-11T02:56:03.418688Z","shell.execute_reply":"2020-11-11T02:56:03.41955Z"},"id":"Wdd0A5zbZnMo","papermill":{"duration":0.558661,"end_time":"2020-11-11T02:56:03.419766","exception":false,"start_time":"2020-11-11T02:56:02.861105","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["from sklearn.feature_selection import VarianceThreshold\n","\n","if VARIANCE_THRESHOLD:\n","    vt_cols = _X_train.loc[:,GENES+CELLS].columns\n","\n","    vh = VarianceThreshold(0.8)\n","    train_trans = pd.DataFrame(vh.fit_transform(_X_train.loc[:,GENES+CELLS]))\n","    vt_cols = vt_cols[vh.get_support()]\n","\n","\n","    vt_cols_leaky = _X_train.loc[:,GENES+CELLS].columns\n","\n","    vh_leaky = VarianceThreshold(0.8)\n","    train_trans = pd.DataFrame(vh_leaky.fit_transform(pd.concat([_X_train.loc[:,GENES+CELLS], _X_test.loc[:,GENES+CELLS]])))\n","    vt_cols_leaky = vt_cols_leaky[vh_leaky.get_support()]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2020-11-11T02:56:03.586374Z","iopub.status.busy":"2020-11-11T02:56:03.585547Z","iopub.status.idle":"2020-11-11T02:56:03.881452Z","shell.execute_reply":"2020-11-11T02:56:03.882746Z"},"id":"J130AzNKZnMr","papermill":{"duration":0.391116,"end_time":"2020-11-11T02:56:03.882959","exception":false,"start_time":"2020-11-11T02:56:03.491843","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["def deal_with_cp_type(_X_train, _X_test):\n","    train_trt_index = _X_train.cp_type == 'trt_cp'\n","    test_trt_index = _X_test.cp_type == 'trt_cp'\n","\n","    _X_train = _X_train[train_trt_index]\n","    _X_test = _X_test\n","    del _X_train['cp_type']\n","    del _X_test['cp_type']\n","\n","    return _X_train, _X_test, train_trt_index, test_trt_index\n","\n","_X_train, _X_test, train_trt_index, test_trt_index = deal_with_cp_type(_X_train, _X_test)\n","\n","\n","def deal_with_cp_dose(_X_train, _X_test):\n","    _X_train = pd.concat([_X_train, pd.get_dummies(_X_train.cp_dose)], axis=1)\n","    del _X_train['cp_dose']\n","    _X_test = pd.concat([_X_test, pd.get_dummies(_X_test.cp_dose)], axis=1)\n","    del _X_test['cp_dose']\n","\n","    return _X_train, _X_test\n","\n","_X_train, _X_test = deal_with_cp_dose(_X_train, _X_test)\n","\n","\n","def deal_with_sig_id(_X_train, _X_test):\n","    del _X_train['sig_id']\n","    del _X_test['sig_id']\n","\n","    return _X_train, _X_test\n","\n","_X_train, _X_test = deal_with_sig_id(_X_train, _X_test)\n","\n","\n","def deal_with_y(_y_train, train_trt_index):\n","    _y_train = _y_train[train_trt_index]\n","    del _y_train['sig_id']\n","\n","    return _y_train\n","\n","_y_train = deal_with_y(_y_train, train_trt_index)\n","\n","\n","\n","BASE_COLS = ['cp_time', 'D1', 'D2']\n","\n","_X_train_dae = _X_train.copy()\n","_X_test_dae = _X_test.copy()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2020-11-11T02:56:04.038787Z","iopub.status.busy":"2020-11-11T02:56:04.038023Z","iopub.status.idle":"2020-11-11T02:56:04.103862Z","shell.execute_reply":"2020-11-11T02:56:04.105177Z"},"id":"gbE1kE4yZnMv","papermill":{"duration":0.152255,"end_time":"2020-11-11T02:56:04.105392","exception":false,"start_time":"2020-11-11T02:56:03.953137","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["_X_train = _X_train.reset_index()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2020-11-11T02:56:04.250803Z","iopub.status.busy":"2020-11-11T02:56:04.25001Z","iopub.status.idle":"2020-11-11T02:56:05.530602Z","shell.execute_reply":"2020-11-11T02:56:05.529511Z"},"id":"xb8WkeFHZnMz","papermill":{"duration":1.353884,"end_time":"2020-11-11T02:56:05.530721","exception":false,"start_time":"2020-11-11T02:56:04.176837","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["non_scored_ones = pd.DataFrame()\n","non_scored_ones['col_name'] = ''\n","non_scored_ones['item_counts'] = 0\n","\n","for col in train_targets_nonscored.columns[1:]:\n","\n","    item_counts = len(train_targets_nonscored[train_targets_nonscored[col] == 1])\n","\n","    non_scored_ones = non_scored_ones.append({'col_name':col, 'item_counts':item_counts}, ignore_index=True)\n","\n","non_scored_target_cols = non_scored_ones[non_scored_ones.item_counts > 10].col_name\n","\n","y_non_train = train_targets_nonscored[non_scored_target_cols]\n","y_non_train = y_non_train[train_trt_index].reset_index(drop=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2020-11-11T02:56:05.642886Z","iopub.status.busy":"2020-11-11T02:56:05.637745Z","iopub.status.idle":"2020-11-11T02:56:09.962092Z","shell.execute_reply":"2020-11-11T02:56:09.961273Z"},"id":"yfbtG00QZnM1","papermill":{"duration":4.383797,"end_time":"2020-11-11T02:56:09.962206","exception":false,"start_time":"2020-11-11T02:56:05.578409","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["from sklearn.decomposition import PCA\n","\n","def yield_pca(_X_train, _X_test, prefix, decomp_cols, comp=50, random_state=42):\n","    pca = PCA(n_components=comp, random_state=random_state)\n","    pca.fit(pd.concat([_X_train.loc[:,decomp_cols], _X_test.loc[:,decomp_cols]]))\n","    _X_train_PCA = pca.transform(_X_train[decomp_cols])\n","    _X_test_PCA = pca.transform(_X_test[decomp_cols])\n","\n","    _X_train_PCA = pd.DataFrame(_X_train_PCA, columns=[f'pca_{prefix}-{i}' for i in range(comp)])\n","    _X_test_PCA = pd.DataFrame(_X_test_PCA, columns=[f'pca_{prefix}-{i}' for i in range(comp)])\n","\n","    return _X_train_PCA, _X_test_PCA\n","\n","_X_train_G_PCA, _X_test_G_PCA = yield_pca(_X_train, _X_test, 'G', GENES, 90)\n","_X_train_C_PCA, _X_test_C_PCA = yield_pca(_X_train, _X_test, 'C', CELLS, 27)\n","_X_train_G_PCA_Dense, _X_test_G_PCA_Dense = yield_pca(_X_train, _X_test, 'Gd', GENES, 8)\n","_X_train_C_PCA_Dense, _X_test_C_PCA_Dense = yield_pca(_X_train, _X_test, 'Cd', CELLS, 1)\n","\n","_X_train = pd.concat([_X_train, _X_train_G_PCA], axis=1)\n","_X_train = pd.concat([_X_train, _X_train_C_PCA], axis=1)\n","_X_train = pd.concat([_X_train, _X_train_G_PCA_Dense], axis=1)\n","_X_train = pd.concat([_X_train, _X_train_C_PCA_Dense], axis=1)\n","\n","_X_test = pd.concat([_X_test, _X_test_G_PCA], axis=1)\n","_X_test = pd.concat([_X_test, _X_test_C_PCA], axis=1)\n","_X_test = pd.concat([_X_test, _X_test_G_PCA_Dense], axis=1)\n","_X_test = pd.concat([_X_test, _X_test_C_PCA_Dense], axis=1)\n","\n","# cols\n","PCA_G = [col for col in _X_train.columns if col.startswith('pca_G-')] + [col for col in _X_train.columns if col.startswith('pca_Gd-')]\n","PCA_C = [col for col in _X_train.columns if col.startswith('pca_C-')] + [col for col in _X_train.columns if col.startswith('pca_Cd-')]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2020-11-11T02:56:10.072523Z","iopub.status.busy":"2020-11-11T02:56:10.071234Z","iopub.status.idle":"2020-11-11T02:56:15.943178Z","shell.execute_reply":"2020-11-11T02:56:15.943767Z"},"id":"VGcbf95XZnM4","papermill":{"duration":5.934346,"end_time":"2020-11-11T02:56:15.943907","exception":false,"start_time":"2020-11-11T02:56:10.009561","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["def make_feature(data, target_feats, create, prefix):\n","    target_data = data.loc[:,target_feats]\n","    invoker = getattr(target_data, create)\n","    return pd.DataFrame(invoker(axis=1), columns=[f'{prefix}_{create}'])\n","\n","for method in ['sum', 'mean', 'std', 'skew', 'kurt', 'median']: # min max\n","    _X_train = pd.concat([_X_train, make_feature(_X_train, GENES+CELLS, method, 'gce')], axis = 1)\n","    _X_train = pd.concat([_X_train, make_feature(_X_train, GENES, method, 'ge')], axis = 1)\n","\n","    _X_test = pd.concat([_X_test, make_feature(_X_test, GENES+CELLS, method, 'gce')], axis = 1)\n","    _X_test = pd.concat([_X_test, make_feature(_X_test, GENES, method, 'ge')], axis = 1)\n","\n","GC_EX = [col for col in _X_train.columns if col.startswith('gce_')]\n","G_EX = [col for col in _X_train.columns if col.startswith('ge_')]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2020-11-11T02:56:16.100552Z","iopub.status.busy":"2020-11-11T02:56:16.099575Z","iopub.status.idle":"2020-11-11T02:56:42.558204Z","shell.execute_reply":"2020-11-11T02:56:42.557586Z"},"id":"OE0XQx6CZnM6","papermill":{"duration":26.538697,"end_time":"2020-11-11T02:56:42.558332","exception":false,"start_time":"2020-11-11T02:56:16.019635","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["from sklearn.preprocessing import StandardScaler\n","from sklearn.preprocessing import QuantileTransformer\n","\n","train_col_names = _X_train.columns\n","test_col_names = _X_test.columns\n","\n","TRANSFORM_TARGET_COLS = BASE_COLS + GENES + CELLS + PCA_G + PCA_C + G_EX + GC_EX\n","\n","\n","def rank_gauss(_X_train, _X_test, cols=TRANSFORM_TARGET_COLS, random_state=42):\n","    qt = QuantileTransformer(random_state=random_state, output_distribution='normal')\n","    qt.fit(pd.concat([_X_train[cols], _X_test[cols]]))\n","    _X_train[cols] = qt.transform(_X_train[cols])\n","    _X_test[cols] = qt.transform(_X_test[cols])\n","\n","    return _X_train, _X_test\n","\n","\n","def standard_scaler(_X_train, _X_test, cols=TRANSFORM_TARGET_COLS):\n","    ss = StandardScaler()\n","    ss.fit(pd.concat([_X_train[cols], _X_test[cols]]))\n","    _X_train[cols] = ss.transform(_X_train[cols])\n","    _X_test[cols] = ss.transform(_X_test[cols])\n","\n","    return _X_train, _X_test\n","\n","_X_train, _X_test = rank_gauss(_X_train, _X_test)\n","_X_train, _X_test = standard_scaler(_X_train, _X_test)\n","\n","_X_train.columns = train_col_names\n","_X_test.columns = test_col_names"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2020-11-11T02:56:42.714154Z","iopub.status.busy":"2020-11-11T02:56:42.712705Z","iopub.status.idle":"2020-11-11T02:56:42.787706Z","shell.execute_reply":"2020-11-11T02:56:42.787123Z"},"id":"9ml5GXacZnM8","papermill":{"duration":0.180852,"end_time":"2020-11-11T02:56:42.787816","exception":false,"start_time":"2020-11-11T02:56:42.606964","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["if VARIANCE_THRESHOLD:\n","    vt_cols = vt_cols.values.tolist() + BASE_COLS + PCA_G + PCA_C + GC_EX + G_EX\n","\n","    _X_train_nl = pd.concat([_X_train.iloc[:,:1], _X_train.loc[:,vt_cols]], axis=1)\n","    _X_test_nl = _X_test.loc[:,vt_cols]\n","    \n","        \n","    vt_cols_leaky = vt_cols_leaky.values.tolist() + BASE_COLS + PCA_G + PCA_C + GC_EX + G_EX\n","\n","    _X_train = pd.concat([_X_train.iloc[:,:1], _X_train.loc[:,vt_cols_leaky]], axis=1)\n","    _X_test = _X_test.loc[:,vt_cols_leaky]\n","\n","    GENES = [col for col in _X_train.columns if col.startswith('g-')]\n","    CELLS = [col for col in _X_train.columns if col.startswith('c-')]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2020-11-11T02:56:42.888997Z","iopub.status.busy":"2020-11-11T02:56:42.887749Z","iopub.status.idle":"2020-11-11T02:56:43.039771Z","shell.execute_reply":"2020-11-11T02:56:43.039236Z"},"id":"XeRC_OxIZnM_","papermill":{"duration":0.204125,"end_time":"2020-11-11T02:56:43.039886","exception":false,"start_time":"2020-11-11T02:56:42.835761","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["_X_train = _X_train.set_index('index')\n","X_train = _X_train.copy()\n","X_test = _X_test.copy()\n","\n","_X_train_nl = _X_train_nl.set_index('index')\n","X_train_nl = _X_train_nl.copy()\n","X_test_nl = _X_test_nl.copy()\n","\n","y_train = _y_train.copy()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2020-11-11T02:56:43.163983Z","iopub.status.busy":"2020-11-11T02:56:43.163125Z","iopub.status.idle":"2020-11-11T02:56:43.166771Z","shell.execute_reply":"2020-11-11T02:56:43.166239Z"},"id":"YEr8LN7oZnND","papermill":{"duration":0.078846,"end_time":"2020-11-11T02:56:43.166872","exception":false,"start_time":"2020-11-11T02:56:43.088026","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["import time\n","\n","def train_and_predict(name, model_func, _X_i_train, _y_i_train, _X_i_test, orig_targets, result_template,\n","                      seeds, splits, epochs, batch_size, shuffle_rows=False, pick_col_size=800, \n","                      do_show_metrics=True, show_each_metrics=True):\n","    st = time.time()\n","\n","    is_list = isinstance(_X_i_train, list)\n","\n","    val_result = orig_targets.copy()\n","    val_result.loc[:, :] = 0\n","\n","    sub_result = result_template.copy()\n","    sub_result.loc[:, 1:] = 0\n","\n","    for h, seed in enumerate(seeds):\n","\n","        seed_everything(seed)\n","\n","        for i, (train_idx, valid_idx) in enumerate(MultilabelStratifiedKFold(n_splits=splits, random_state=seed, shuffle=True)\n","                                        .split(_y_i_train, _y_i_train)):\n","            print(f'Fold {i+1}')\n","\n","            if is_list:\n","                _X_train = [_X_i_train[0].loc[:,:].values[train_idx], _X_i_train[1].loc[:,:].values[train_idx]]\n","                _X_valid = [_X_i_train[0].loc[:,:].values[valid_idx], _X_i_train[1].loc[:,:].values[valid_idx]]\n","            else:\n","                _X_train = _X_i_train.loc[:,:].values[train_idx]\n","                _X_valid = _X_i_train.loc[:,:].values[valid_idx]\n","\n","            _y_train = _y_i_train.values[train_idx]\n","            _y_valid = _y_i_train.values[valid_idx]\n","\n","            if is_list:\n","                model = model_func(len(_X_i_train[0].columns), len(_X_i_train[1].columns))\n","            else:\n","                model = model_func(len(_X_i_train.columns))\n","            \n","            model.fit(_X_train, _y_train,\n","                    validation_data=(_X_valid, _y_valid),\n","                    epochs=epochs, batch_size=batch_size,\n","                    callbacks=[\n","                        ReduceLROnPlateau(monitor='val_logloss', factor=0.1, patience=3, verbose=1, min_delta=1e-4, mode='min')\n","                        , ModelCheckpoint(f'{name}_{seed}_{i}.hdf5', monitor = 'val_logloss', verbose = 0, save_best_only = True, save_weights_only = True, mode = 'min')\n","                        , EarlyStopping(monitor = 'val_logloss', min_delta = 1e-4, patience = 5, mode = 'min', baseline = None, restore_best_weights = True)\n","                    ], verbose=2)\n","        \n","            model.load_weights(f'{name}_{seed}_{i}.hdf5')\n","            val_result.iloc[_y_i_train.iloc[valid_idx,:].index.values, :] += model.predict(_X_valid)\n","\n","            if is_list:\n","                sub_result.loc[test_trt_index, sub_result.columns[1:]] += model.predict([_X_i_test[0].loc[test_trt_index, :], _X_i_test[1].loc[test_trt_index, :]])\n","            else:\n","                sub_result.loc[test_trt_index, sub_result.columns[1:]] += model.predict(_X_i_test.loc[test_trt_index, :])\n","\n","            print('')\n","\n","        tmp_result = val_result.copy()\n","        tmp_result.iloc[:,1:] = val_result.iloc[:,1:] / (h + 1)\n","        print(f' ---- seed:{seed}, ensemble:{h + 1}')\n","        if do_show_metrics:\n","            _ = show_metrics(tmp_result, show_each=False)\n","\n","    val_result.iloc[:,1:] = val_result.iloc[:,1:] / len(seeds)\n","    if do_show_metrics:\n","        metrics = show_metrics(val_result, show_each=show_each_metrics)\n","    else:\n","        metrics = None\n","\n","    sub_result.iloc[:, 1:] = sub_result.iloc[:, 1:] / (len(seeds) * splits)\n","\n","    print(f' elapsed: {time.time() - st}')\n","\n","    return sub_result, val_result, metrics"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.0473,"end_time":"2020-11-11T02:56:43.263102","exception":false,"start_time":"2020-11-11T02:56:43.215802","status":"completed"},"tags":[]},"source":["## Make Extra Features from Non-Scored Targets\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2020-11-11T02:56:43.363434Z","iopub.status.busy":"2020-11-11T02:56:43.361569Z","iopub.status.idle":"2020-11-11T02:56:43.364117Z","shell.execute_reply":"2020-11-11T02:56:43.364602Z"},"id":"x47pVGS7ZnNF","papermill":{"duration":0.055293,"end_time":"2020-11-11T02:56:43.364717","exception":false,"start_time":"2020-11-11T02:56:43.309424","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["p_min = 0.001\n","p_max = 0.999\n","\n","def logloss(y_true, y_pred):\n","    y_pred = tf.clip_by_value(y_pred,p_min,p_max)\n","    return -K.mean(y_true*K.log(y_pred) + (1-y_true)*K.log(1-y_pred))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2020-11-11T02:56:43.466269Z","iopub.status.busy":"2020-11-11T02:56:43.465443Z","iopub.status.idle":"2020-11-11T02:56:43.474574Z","shell.execute_reply":"2020-11-11T02:56:43.475064Z"},"id":"Ot070H1DZnNH","papermill":{"duration":0.062737,"end_time":"2020-11-11T02:56:43.475184","exception":false,"start_time":"2020-11-11T02:56:43.412447","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["output_bias=tf.keras.initializers.Constant(-np.log(y_train.mean(axis=0).to_numpy()))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2020-11-11T02:56:43.581323Z","iopub.status.busy":"2020-11-11T02:56:43.580624Z","iopub.status.idle":"2020-11-11T02:56:43.584515Z","shell.execute_reply":"2020-11-11T02:56:43.583988Z"},"id":"DmfoULIqZnNJ","papermill":{"duration":0.062844,"end_time":"2020-11-11T02:56:43.584641","exception":false,"start_time":"2020-11-11T02:56:43.521797","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["def create_add_model(input_dim):\n","    print(f'the input dim is {input_dim}')\n","\n","    model = M.Sequential()\n","    model.add(L.Input(input_dim))\n","    model.add(L.BatchNormalization())\n","    model.add(L.Dropout(0.5))\n","    model.add(WeightNormalization(L.Dense(input_dim, activation='elu')))\n","\n","    model.add(L.BatchNormalization())\n","    model.add(L.Dropout(0.4))\n","    model.add(WeightNormalization(L.Dense(512, activation='selu')))\n","\n","    model.add(L.BatchNormalization())\n","    model.add(L.Dropout(0.5))\n","    model.add(WeightNormalization(L.Dense(y_non_train.shape[1], activation='sigmoid',\n","                                          bias_initializer=tf.keras.initializers.Constant(-np.log(y_non_train.mean(axis=0).to_numpy())\n","                                          ))))\n","\n","    model.compile(optimizer=tfa.optimizers.Lookahead(tf.optimizers.Adam(lr=1e-3), sync_period=10),\n","                loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=LABEL_SMOOTHING), metrics=logloss)\n","\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2020-11-11T02:56:43.684021Z","iopub.status.busy":"2020-11-11T02:56:43.683047Z","iopub.status.idle":"2020-11-11T02:56:43.76704Z","shell.execute_reply":"2020-11-11T02:56:43.766532Z"},"id":"oYngl941ZnNL","papermill":{"duration":0.135668,"end_time":"2020-11-11T02:56:43.76714","exception":false,"start_time":"2020-11-11T02:56:43.631472","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["non_scored_template = sample_submission.copy()\n","non_scored_template = pd.DataFrame(non_scored_template.pop('sig_id'))\n","for col in non_scored_target_cols:\n","    non_scored_template[col] = 0."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2020-11-11T02:56:43.879837Z","iopub.status.busy":"2020-11-11T02:56:43.878858Z","iopub.status.idle":"2020-11-11T03:05:47.924304Z","shell.execute_reply":"2020-11-11T03:05:47.924982Z"},"id":"NV5mifhHZnNN","outputId":"a725d6c5-b7a5-4a88-a4a2-d26c804f9b15","papermill":{"duration":544.110819,"end_time":"2020-11-11T03:05:47.925152","exception":false,"start_time":"2020-11-11T02:56:43.814333","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["if ADD_NON_TRAIN:\n","    add_result, add_valid_preds, add_metrics = train_and_predict('ADD', create_add_model, X_train, y_non_train, X_test,\n","                                                                 train_targets_nonscored[non_scored_target_cols].loc[train_trt_index,:], non_scored_template,\n","                                                                  ADD_SEEDS, ADD_SPLITS, ADD_EPOCHS, ADD_BATCH_SIZE, shuffle_rows=False, do_show_metrics=False, show_each_metrics=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2020-11-11T03:05:48.533428Z","iopub.status.busy":"2020-11-11T03:05:48.531983Z","iopub.status.idle":"2020-11-11T03:05:50.731825Z","shell.execute_reply":"2020-11-11T03:05:50.731256Z"},"id":"nFd6u4NmZnNP","papermill":{"duration":2.467582,"end_time":"2020-11-11T03:05:50.731963","exception":false,"start_time":"2020-11-11T03:05:48.264381","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["if ADD_NON_TRAIN:\n","    X_non_train = add_valid_preds.copy()\n","    X_non_test = add_result.iloc[:,1:].copy()\n","    cols = X_non_train.columns\n","    \n","    X_non_train, X_non_test = rank_gauss(X_non_train, X_non_test, cols)\n","    X_non_train, X_non_test = standard_scaler(X_non_train, X_non_test, cols)"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.189829,"end_time":"2020-11-11T03:05:51.110835","exception":false,"start_time":"2020-11-11T03:05:50.921006","status":"completed"},"tags":[]},"source":["## Shallow NN with DenoisingAutoEncoder\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2020-11-11T03:05:51.496819Z","iopub.status.busy":"2020-11-11T03:05:51.495978Z","iopub.status.idle":"2020-11-11T03:05:51.50005Z","shell.execute_reply":"2020-11-11T03:05:51.499388Z"},"id":"T5EnSY0FbqAP","papermill":{"duration":0.199361,"end_time":"2020-11-11T03:05:51.500165","exception":false,"start_time":"2020-11-11T03:05:51.300804","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["def create_autoencoder(input_dim):\n","    input_vector = L.Input(shape=(input_dim,))\n","    encoded = L.Dense(1024, activation='elu')(input_vector)\n","    encoded = L.Dense(2048, activation='elu')(encoded)\n","    decoded = L.Dense(1024, activation='elu')(encoded)\n","    decoded = L.Dense(input_dim, activation='elu')(decoded)\n","    \n","    autoencoder = M.Model(input_vector, decoded)\n","    autoencoder.compile(optimizer=tf.optimizers.Adam(lr=0.0012, amsgrad=True), loss='mse')\n","    \n","    return autoencoder"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2020-11-11T03:05:51.940097Z","iopub.status.busy":"2020-11-11T03:05:51.93918Z","iopub.status.idle":"2020-11-11T03:07:19.51377Z","shell.execute_reply":"2020-11-11T03:07:19.512422Z"},"id":"Gq3jxbOobpZo","outputId":"7e753478-aaf0-449e-a262-d8de6f19a726","papermill":{"duration":87.809095,"end_time":"2020-11-11T03:07:19.513888","exception":false,"start_time":"2020-11-11T03:05:51.704793","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["mu, sigma = 0, 0.1\n","\n","seed_everything(128)\n","\n","dae_train = pd.concat([_X_train_dae, _X_test_dae])\n","\n","noise = np.random.normal(mu, sigma, [dae_train.shape[0], dae_train.shape[1]]) \n","noised_train = dae_train + noise\n","\n","autoencoder = create_autoencoder(dae_train.shape[1])\n","\n","autoencoder.fit(noised_train, dae_train,\n","                epochs=500,\n","                batch_size=128,\n","                callbacks=[\n","                    ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience = 3, verbose=1, min_delta=1e-4, mode='min')\n","                    , ModelCheckpoint(f'dae.hdf5', monitor = 'val_loss', verbose = 0, save_best_only = True, save_weights_only = True, mode = 'min')\n","                    , EarlyStopping(monitor = 'val_loss', min_delta = 1e-4, patience = 8, mode = 'min', baseline = None, restore_best_weights = True)],\n","                shuffle=True,\n","                validation_split=0.2)\n","\n","autoencoder.load_weights(f'dae.hdf5')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2020-11-11T03:07:20.935214Z","iopub.status.busy":"2020-11-11T03:07:20.934223Z","iopub.status.idle":"2020-11-11T03:07:22.161322Z","shell.execute_reply":"2020-11-11T03:07:22.160729Z"},"id":"KyJltOnOiZMC","papermill":{"duration":1.921193,"end_time":"2020-11-11T03:07:22.161454","exception":false,"start_time":"2020-11-11T03:07:20.240261","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["\n","encoder = M.Model(autoencoder.input, autoencoder.layers[2].output)\n","__X_train_dae = pd.DataFrame(encoder.predict(_X_train_dae))\n","__X_test_dae = pd.DataFrame(encoder.predict(_X_test_dae))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2020-11-11T03:07:23.543041Z","iopub.status.busy":"2020-11-11T03:07:23.542028Z","iopub.status.idle":"2020-11-11T03:07:25.043286Z","shell.execute_reply":"2020-11-11T03:07:25.042527Z"},"id":"RTDpZjskncWq","outputId":"d99c9dc1-c680-4272-eda3-961e4dfedac8","papermill":{"duration":2.194213,"end_time":"2020-11-11T03:07:25.043404","exception":false,"start_time":"2020-11-11T03:07:22.849191","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["vh_dae = VarianceThreshold(0.2)\n","vh_dae.fit_transform(pd.concat([__X_train_dae, __X_test_dae]))\n","dae_cols = __X_train_dae.columns[vh_dae.get_support()]\n","print(len(dae_cols))\n","\n","X_train_dae = __X_train_dae.loc[:, dae_cols]\n","X_test_dae = __X_test_dae.loc[:, dae_cols]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2020-11-11T03:07:26.426687Z","iopub.status.busy":"2020-11-11T03:07:26.4255Z","iopub.status.idle":"2020-11-11T03:07:26.649333Z","shell.execute_reply":"2020-11-11T03:07:26.648574Z"},"id":"y6wJ0AcsDgXN","papermill":{"duration":0.920651,"end_time":"2020-11-11T03:07:26.649461","exception":false,"start_time":"2020-11-11T03:07:25.72881","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["X_train_dae = pd.concat([X_train_dae, X_non_train.reset_index(drop=True)], axis=1)\n","X_test_dae = pd.concat([X_test_dae, X_non_test.reset_index(drop=True)], axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2020-11-11T03:07:28.338082Z","iopub.status.busy":"2020-11-11T03:07:28.331492Z","iopub.status.idle":"2020-11-11T03:09:04.435467Z","shell.execute_reply":"2020-11-11T03:09:04.434723Z"},"id":"lWnMD9lRea6u","papermill":{"duration":96.850126,"end_time":"2020-11-11T03:09:04.435681","exception":false,"start_time":"2020-11-11T03:07:27.585555","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["_X_train_G_PCA_dae, _X_test_G_PCA_dae = yield_pca(_X_train, _X_test, 'Gd', GENES, 90, 128)\n","_X_train_C_PCA_dae, _X_test_C_PCA_dae = yield_pca(_X_train, _X_test, 'Cd', CELLS, 27, 128)\n","\n","_X_train_G_PCA_Dense_dae, _X_test_G_PCA_Dense_dae = yield_pca(_X_train, _X_test, 'Gd', GENES, 8, 128)\n","_X_train_C_PCA_Dense_dae, _X_test_C_PCA_Dense_dae = yield_pca(_X_train, _X_test, 'Cd', CELLS, 1, 128)\n","\n","X_train_dae = pd.concat([X_train_dae, _X_train_G_PCA_dae, _X_train_C_PCA_dae, _X_train_G_PCA_Dense, _X_train_C_PCA_Dense_dae], axis=1)\n","X_test_dae = pd.concat([X_test_dae, _X_test_G_PCA_dae, _X_test_C_PCA_dae, _X_test_G_PCA_Dense, _X_test_C_PCA_Dense_dae], axis=1)\n","\n","X_train_dae, X_test_dae = rank_gauss(X_train_dae, X_test_dae, X_train_dae.columns, random_state=128)\n","X_train_dae, X_test_dae = standard_scaler(X_train_dae, X_test_dae, X_train_dae.columns)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2020-11-11T03:09:05.886815Z","iopub.status.busy":"2020-11-11T03:09:05.88601Z","iopub.status.idle":"2020-11-11T03:09:05.889325Z","shell.execute_reply":"2020-11-11T03:09:05.88881Z"},"id":"Vl5gCMgvj9A8","papermill":{"duration":0.728341,"end_time":"2020-11-11T03:09:05.889424","exception":false,"start_time":"2020-11-11T03:09:05.161083","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["def create_simple_nn(input_dim):\n","    print(f'the input dim is {input_dim}')\n","\n","    model = M.Sequential()\n","    model.add(L.Input(input_dim))\n","    model.add(L.BatchNormalization())\n","    model.add(L.Dropout(0.5))\n","    model.add(WeightNormalization(L.Dense(input_dim, activation='selu')))\n","\n","    model.add(L.BatchNormalization())\n","    model.add(L.Dropout(0.5))\n","    model.add(WeightNormalization(L.Dense(512, activation='swish')))\n","\n","    model.add(L.BatchNormalization())\n","    model.add(L.Dropout(0.5))\n","    model.add(WeightNormalization(L.Dense(206, activation='sigmoid', bias_initializer=output_bias)))\n","\n","    model.compile(optimizer=tfa.optimizers.Lookahead(tf.optimizers.Adam(lr=0.01), sync_period=10),\n","                loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=LABEL_SMOOTHING), metrics=logloss)\n","\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2020-11-11T03:09:07.602581Z","iopub.status.busy":"2020-11-11T03:09:07.60113Z","iopub.status.idle":"2020-11-11T03:20:48.950106Z","shell.execute_reply":"2020-11-11T03:20:48.948543Z"},"id":"M3x9j-PrkCEu","outputId":"fe687efb-7f77-44c5-e054-6cd7ee067bb2","papermill":{"duration":702.289442,"end_time":"2020-11-11T03:20:48.950228","exception":false,"start_time":"2020-11-11T03:09:06.660786","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["if RUN_SNN:\n","    snn_result, snn_valid_preds, snn_metrics = train_and_predict('SNN', create_simple_nn, X_train_dae, _y_train, X_test_dae,\n","                                                                 train_targets_scored.iloc[:,1:], sample_submission,\n","                                                                 SNN_SEEDS, SNN_SPLITS, SNN_EPOCHS, SNN_BATCH_SIZE, shuffle_rows=False)"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":1.127374,"end_time":"2020-11-11T03:20:50.987046","exception":false,"start_time":"2020-11-11T03:20:49.859672","status":"completed"},"tags":[]},"source":["## ResNet\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2020-11-11T03:20:52.876057Z","iopub.status.busy":"2020-11-11T03:20:52.87479Z","iopub.status.idle":"2020-11-11T03:20:52.957588Z","shell.execute_reply":"2020-11-11T03:20:52.956957Z"},"id":"CjUvJrq1ZnNQ","papermill":{"duration":1.050136,"end_time":"2020-11-11T03:20:52.957714","exception":false,"start_time":"2020-11-11T03:20:51.907578","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["_X_train = pd.concat([X_train, X_non_train], axis=1)\n","_X_test = pd.concat([X_test, X_non_test], axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2020-11-11T03:20:54.751149Z","iopub.status.busy":"2020-11-11T03:20:54.749763Z","iopub.status.idle":"2020-11-11T03:20:54.977886Z","shell.execute_reply":"2020-11-11T03:20:54.977332Z"},"id":"27GFhFdRZnNT","papermill":{"duration":1.118921,"end_time":"2020-11-11T03:20:54.978011","exception":false,"start_time":"2020-11-11T03:20:53.85909","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["sep_cols = BASE_COLS + GENES #+ CELLS\n","_X_train_1 = _X_train[sep_cols]\n","_X_train_2 = _X_train.drop(sep_cols,axis=1)\n","\n","_X_test_1 = _X_test[sep_cols]\n","_X_test_2 = _X_test.drop(sep_cols,axis=1)\n","\n","_X_train = [_X_train_1, _X_train_2]\n","_X_test = [_X_test_1, _X_test_2]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2020-11-11T03:20:56.733281Z","iopub.status.busy":"2020-11-11T03:20:56.732382Z","iopub.status.idle":"2020-11-11T03:20:56.736176Z","shell.execute_reply":"2020-11-11T03:20:56.735636Z"},"id":"uyuvoXwOZnNV","papermill":{"duration":0.895912,"end_time":"2020-11-11T03:20:56.736283","exception":false,"start_time":"2020-11-11T03:20:55.840371","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["def create_res_model(n_features, n_features_2):\n","    print(f'the input dim is {n_features}, {n_features_2}')\n","\n","    input_1 = L.Input(shape = (n_features,), name = 'Input1')\n","    input_2 = L.Input(shape = (n_features_2,), name = 'Input2')\n","\n","    head_1 = M.Sequential([\n","        L.BatchNormalization(),\n","        L.Dropout(0.3),\n","        L.Dense(512, activation='elu'), \n","        L.BatchNormalization(),\n","        L.Dropout(0.5),\n","        L.Dense(256, activation='elu')\n","        ],name='Head1') \n","\n","    input_3 = head_1(input_1)\n","    input_3_concat = L.Concatenate()([input_2, input_3])\n","\n","    head_2 = M.Sequential([\n","        L.BatchNormalization(),\n","        L.Dropout(0.3),\n","        L.Dense(n_features_2, activation='relu'),\n","        L.BatchNormalization(),\n","        L.Dropout(0.5),\n","        L.Dense(n_features_2, activation='elu'),\n","        L.BatchNormalization(),\n","        L.Dropout(0.5),\n","        L.Dense(256, activation='relu'),\n","        L.BatchNormalization(),\n","        L.Dropout(0.5),\n","        L.Dense(256, activation='selu')\n","        ],name='Head2')\n","\n","    input_4 = head_2(input_3_concat)\n","    input_4_avg = L.Average()([input_3, input_4]) \n","\n","    head_3 = M.Sequential([\n","        L.BatchNormalization(),\n","        L.Dropout(0.3),\n","        L.Dense(256, activation='swish'),\n","        L.BatchNormalization(),\n","        L.Dense(256, activation='selu'),\n","        L.BatchNormalization(),\n","        L.Dense(206, activation='sigmoid')\n","        ],name='Head3')\n","\n","    output = head_3(input_4_avg)\n","\n","\n","    model = M.Model(inputs = [input_1, input_2], outputs = output)\n","    model.compile(optimizer=tf.optimizers.Adam(lr=0.002),\n","                  loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=LABEL_SMOOTHING), metrics=logloss)\n","\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2020-11-11T03:20:58.515461Z","iopub.status.busy":"2020-11-11T03:20:58.514066Z","iopub.status.idle":"2020-11-11T03:35:02.512875Z","shell.execute_reply":"2020-11-11T03:35:02.51147Z"},"id":"lgDB2jdCZnNX","outputId":"059eab4b-f632-47df-b34a-6f97460b5619","papermill":{"duration":844.89847,"end_time":"2020-11-11T03:35:02.513035","exception":false,"start_time":"2020-11-11T03:20:57.614565","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["if RUN_NN:\n","    nn_result, nn_valid_preds, nn_metrics = train_and_predict('RES', create_res_model, _X_train, y_train, _X_test,\n","                                                              train_targets_scored.iloc[:,1:], sample_submission,\n","                                                              NN_SEEDS, NN_SPLITS, NN_EPOCHS, NN_BATCH_SIZE, shuffle_rows=False)"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":1.437608,"end_time":"2020-11-11T03:35:05.097169","exception":false,"start_time":"2020-11-11T03:35:03.659561","status":"completed"},"tags":[]},"source":["## Two Headed NN\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2020-11-11T03:35:07.44259Z","iopub.status.busy":"2020-11-11T03:35:07.440987Z","iopub.status.idle":"2020-11-11T03:35:07.56787Z","shell.execute_reply":"2020-11-11T03:35:07.567297Z"},"id":"d2rZymdDZnNZ","papermill":{"duration":1.285973,"end_time":"2020-11-11T03:35:07.568028","exception":false,"start_time":"2020-11-11T03:35:06.282055","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["if ADD_NON_TRAIN:\n","    _X_train = []\n","    _X_train.append(X_train.copy())\n","    _X_train.append(X_non_train.copy())\n","\n","    _X_test = []\n","    _X_test.append(X_test.copy())\n","    _X_test.append(X_non_test.copy())\n","\n","\n","\n","_X_train[1] = pd.concat([_X_train[1], _X_train[0][CELLS]], axis=1)\n","_X_test[1] = pd.concat([_X_test[1], _X_test[0][CELLS]], axis=1)\n","_X_train[0] = _X_train[0].drop(CELLS, axis=1)\n","_X_test[0] = _X_test[0].drop(CELLS, axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2020-11-11T03:35:10.201615Z","iopub.status.busy":"2020-11-11T03:35:10.199603Z","iopub.status.idle":"2020-11-11T03:35:10.2025Z","shell.execute_reply":"2020-11-11T03:35:10.203145Z"},"id":"5E5pohlcZnNb","papermill":{"duration":1.458214,"end_time":"2020-11-11T03:35:10.203284","exception":false,"start_time":"2020-11-11T03:35:08.74507","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["def create_twohead_model(input_dim1, input_dim2):\n","    print(f'the input dim is {input_dim1}, {input_dim2}')\n","\n","    input_1 = L.Input(input_dim1)\n","    x1 = L.BatchNormalization()(input_1)\n","    x1 = L.Dropout(0.5)(x1)\n","    o1 = L.Dense(input_dim1, activation='elu')(x1)\n","\n","    input_2 = L.Input(input_dim2)\n","    x2 = L.BatchNormalization()(input_2)\n","    x2 = L.Dropout(0.5)(x2)\n","    o2 = L.Dense(input_dim2, activation='elu')(x2)\n","\n","    x = L.Concatenate()([o1, o2])\n","\n","    x = L.BatchNormalization()(x)\n","    x = L.Dropout(0.4)(x)\n","    x = L.Dense(512, activation='selu')(x)\n","    \n","    x = L.BatchNormalization()(x)\n","    x = L.Dropout(0.5)(x)\n","    output = L.Dense(206, activation='sigmoid')(x)\n","\n","    model = M.Model([input_1, input_2], output)\n","\n","    model.compile(optimizer=tfa.optimizers.Lookahead(tf.optimizers.Adam(lr=0.01), sync_period=10),\n","                loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=LABEL_SMOOTHING), metrics=logloss)\n","    \n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2020-11-11T03:35:12.82965Z","iopub.status.busy":"2020-11-11T03:35:12.828315Z","iopub.status.idle":"2020-11-11T03:48:28.20452Z","shell.execute_reply":"2020-11-11T03:48:28.205131Z"},"id":"dPsGhURkZnNd","outputId":"dc0bd6aa-1b69-4eaf-e2c3-af46134f1db5","papermill":{"duration":796.673352,"end_time":"2020-11-11T03:48:28.205314","exception":false,"start_time":"2020-11-11T03:35:11.531962","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["if RUN_NN2:\n","    nn2_result, nn2_valid_preds, nn2_metrics = train_and_predict('NN2', create_twohead_model, _X_train, y_train, _X_test,\n","                                                                 train_targets_scored.iloc[:,1:], sample_submission,\n","                                                                 NN2_SEEDS, NN2_SPLITS, NN2_EPOCHS, NN2_BATCH_SIZE, shuffle_rows=False)"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":1.539779,"end_time":"2020-11-11T03:48:31.148229","exception":false,"start_time":"2020-11-11T03:48:29.60845","status":"completed"},"tags":[]},"source":["## RNN\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2020-11-11T03:48:34.072117Z","iopub.status.busy":"2020-11-11T03:48:34.056413Z","iopub.status.idle":"2020-11-11T03:48:34.090198Z","shell.execute_reply":"2020-11-11T03:48:34.089672Z"},"id":"Vhd0DOCoZnNf","papermill":{"duration":1.414165,"end_time":"2020-11-11T03:48:34.090302","exception":false,"start_time":"2020-11-11T03:48:32.676137","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["top_feats = [   0,    1,    2,    4,    6,    9,   11,   12,   13,   15,   16,\n","         17,   19,   20,   21,   22,   24,   25,   26,   27,   28,   29,\n","         31,   33,   36,   37,   39,   40,   41,   42,   44,   45,   47,\n","         48,   49,   50,   51,   52,   53,   54,   55,   57,   58,   60,\n","         61,   65,   67,   68,   69,   71,   72,   73,   75,   76,   78,\n","         79,   80,   81,   82,   83,   85,   86,   87,   88,   89,   91,\n","         92,   93,   94,   96,   98,   99,  100,  101,  102,  103,  104,\n","        107,  108,  109,  110,  111,  112,  113,  114,  115,  117,  118,\n","        119,  120,  122,  123,  124,  125,  126,  128,  129,  130,  132,\n","        133,  134,  135,  136,  137,  140,  141,  142,  143,  144,  146,\n","        148,  149,  150,  151,  152,  153,  154,  155,  158,  159,  161,\n","        162,  164,  165,  166,  167,  168,  169,  170,  171,  172,  173,\n","        175,  178,  181,  183,  184,  185,  186,  187,  189,  190,  191,\n","        193,  197,  198,  200,  201,  202,  203,  204,  205,  206,  209,\n","        210,  211,  213,  214,  216,  218,  219,  220,  221,  223,  225,\n","        226,  227,  229,  230,  231,  232,  233,  236,  237,  239,  241,\n","        242,  243,  244,  245,  246,  247,  248,  249,  250,  251,  254,\n","        255,  256,  257,  258,  261,  262,  263,  265,  266,  268,  270,\n","        271,  272,  273,  275,  276,  277,  279,  280,  281,  283,  284,\n","        288,  289,  291,  294,  297,  298,  299,  300,  302,  303,  304,\n","        305,  306,  309,  310,  311,  312,  314,  315,  316,  317,  319,\n","        321,  322,  323,  324,  325,  326,  329,  330,  332,  336,  337,\n","        339,  340,  341,  342,  343,  344,  347,  348,  349,  351,  354,\n","        355,  356,  357,  358,  359,  360,  361,  362,  365,  367,  369,\n","        370,  371,  373,  374,  375,  376,  378,  379,  380,  383,  384,\n","        385,  388,  390,  392,  395,  397,  400,  401,  403,  404,  406,\n","        407,  408,  410,  414,  415,  416,  418,  419,  420,  422,  423,\n","        424,  425,  426,  427,  428,  430,  431,  433,  434,  436,  438,\n","        439,  441,  443,  446,  447,  448,  450,  451,  452,  454,  455,\n","        457,  461,  462,  463,  464,  466,  468,  469,  470,  473,  474,\n","        475,  477,  478,  479,  481,  482,  483,  484,  487,  488,  489,\n","        492,  495,  497,  501,  502,  503,  505,  506,  508,  509,  510,\n","        511,  512,  513,  514,  516,  517,  519,  522,  523,  524,  525,\n","        527,  528,  529,  531,  538,  543,  546,  548,  549,  550,  552,\n","        553,  554,  555,  558,  559,  560,  561,  562,  563,  564,  565,\n","        567,  568,  569,  572,  573,  574,  575,  576,  579,  580,  583,\n","        585,  587,  588,  591,  592,  595,  596,  597,  599,  600,  603,\n","        610,  611,  612,  613,  614,  617,  619,  620,  621,  623,  624,\n","        625,  627,  628,  629,  632,  634,  635,  636,  637,  638,  639,\n","        640,  642,  643,  644,  645,  646,  647,  649,  650,  651,  654,\n","        656,  659,  660,  661,  662,  663,  665,  669,  671,  672,  673,\n","        674,  676,  677,  678,  680,  681,  682,  685,  686,  687,  689,\n","        692,  694,  695,  696,  697,  698,  699,  701,  702,  704,  705,\n","        706,  708,  709,  710,  714,  716,  717,  718,  722,  724,  725,\n","        729,  732,  733,  734,  735,  736,  737,  738,  742,  743,  748,\n","        749,  751,  753,  754,  755,  756,  757,  759,  761,  763,  764,\n","        765,  767,  768,  771,  772,  773,  774,  775,  776,  778,  779,\n","        781,  782,  784,  786,  788,  791,  792,  793,  794,  800,  801,\n","        802,  805,  807,  814,  815,  816,  820,  821,  822,  823,  824,\n","        825,  828,  829,  830,  831,  832,  833,  834,  835,  836,  837,\n","        838,  839,  840,  841,  842,  843,  844,  846,  847,  848,  849,\n","        850,  851,  852,  853,  854,  855,  856,  857,  858,  859,  860,\n","        861,  863,  865,  866,  867,  868,  869,  870,  871,  872,  873,\n","        876,  877,  878,  879,  880,  881,  882,  883,  884,  886,  887,\n","        889,  890,  892,  893,  894,  895,  896,  897,  898,  899,  901,\n","        902,  904,  905,  906,  909,  910,  911,  912,  913,  914,  915,\n","        916,  917,  918,  919,  920,  921,  922,  923,  924,  925,  926,\n","        927,  928,  929,  930,  931,  932,  933,  934,  938,  941,  942,\n","        944,  948,  949,  950,  951,  954,  955,  956,  957,  958,  959,\n","        960,  961,  962,  963,  964,  965,  966,  967,  968,  969,  970,\n","        971,  972,  974,  976,  980,  981,  985,  987,  988,  989,  990,\n","        991,  993,  995,  996,  997,  998,  999, 1000, 1001, 1002, 1003,\n","       1004, 1005, 1006, 1010, 1011, 1012, 1013, 1014, 1016, 1017, 1018,\n","       1019, 1020, 1021, 1023, 1024, 1025, 1026, 1027, 1028, 1029, 1031,\n","       1032, 1034, 1035, 1037, 1038, 1040, 1041, 1042, 1046, 1047, 1048,\n","       1049, 1050, 1051, 1053, 1055, 1056, 1057, 1058, 1059, 1060, 1062,\n","       1064, 1065, 1066, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1076,\n","       1078, 1079, 1080, 1082, 1083, 1084, 1085, 1086, 1087, 1089, 1092,\n","       1093, 1094, 1095, 1096, 1097, 1099, 1100, 1101, 1102, 1103, 1104,\n","       1105, 1106, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116,\n","       1117, 1118, 1119, 1120, 1121, 1122, 1125, 1126, 1127, 1128, 1131,\n","       1132, 1134, 1136, 1137, 1138, 1140, 1141]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2020-11-11T03:48:37.168841Z","iopub.status.busy":"2020-11-11T03:48:37.16759Z","iopub.status.idle":"2020-11-11T03:48:37.251236Z","shell.execute_reply":"2020-11-11T03:48:37.250626Z"},"id":"eL44n4AZZnNh","papermill":{"duration":1.533445,"end_time":"2020-11-11T03:48:37.251357","exception":false,"start_time":"2020-11-11T03:48:35.717912","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["X_train = pd.concat([X_train_nl, X_non_train], axis=1)\n","X_test = pd.concat([X_test_nl, X_non_test], axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2020-11-11T03:48:40.051799Z","iopub.status.busy":"2020-11-11T03:48:40.050379Z","iopub.status.idle":"2020-11-11T03:48:40.348487Z","shell.execute_reply":"2020-11-11T03:48:40.349254Z"},"id":"3hW-qHTaZnNj","papermill":{"duration":1.707584,"end_time":"2020-11-11T03:48:40.349426","exception":false,"start_time":"2020-11-11T03:48:38.641842","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["X_train_rnn = X_train.iloc[:, top_feats]\n","X_test_rnn = X_test.iloc[:, top_feats]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2020-11-11T03:48:43.362529Z","iopub.status.busy":"2020-11-11T03:48:43.360876Z","iopub.status.idle":"2020-11-11T03:48:43.363464Z","shell.execute_reply":"2020-11-11T03:48:43.363961Z"},"id":"ZiqNBAGWZnNk","papermill":{"duration":1.653372,"end_time":"2020-11-11T03:48:43.364087","exception":false,"start_time":"2020-11-11T03:48:41.710715","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["def create_rnn_model(input_dim):\n","    print(f'the input dim is {input_dim}')\n","\n","    model = M.Sequential()\n","    model.add(L.Input(input_dim))\n","    model.add(L.Reshape((1, input_dim)))\n","    model.add(L.BatchNormalization())\n","    model.add(L.Dropout(0.3))\n","    model.add(L.GRU(1024, dropout = 0.5, recurrent_dropout=0.3, return_sequences = True, activation='elu'))\n","    model.add(L.GRU(1024, dropout = 0.5, recurrent_dropout=0.5, activation='selu'))\n","    model.add(L.BatchNormalization())\n","    model.add(L.Dropout(0.5))\n","    model.add(WeightNormalization(L.Dense(206, activation='sigmoid', bias_initializer=output_bias)))\n","    \n","    model.compile(optimizer = tfa.optimizers.Lookahead(tf.optimizers.Adam(0.0015), sync_period = 2), \n","                    loss = tf.keras.losses.BinaryCrossentropy(label_smoothing=LABEL_SMOOTHING), metrics=logloss)\n","    \n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2020-11-11T03:48:46.03678Z","iopub.status.busy":"2020-11-11T03:48:46.035299Z","iopub.status.idle":"2020-11-11T04:09:45.310399Z","shell.execute_reply":"2020-11-11T04:09:45.309841Z"},"id":"_5kydniZZnNm","outputId":"8fc513bb-7298-4084-859c-5655ea340cbc","papermill":{"duration":1260.608798,"end_time":"2020-11-11T04:09:45.310559","exception":false,"start_time":"2020-11-11T03:48:44.701761","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["if RUN_RNN:\n","    rnn_result, rnn_valid_preds, rnn_metrics = train_and_predict('RNN', create_rnn_model, X_train_rnn, y_train, X_test_rnn, \n","                                                                 train_targets_scored.iloc[:,1:], sample_submission,\n","                                                                 RNN_SEEDS, RNN_SPLITS, RNN_EPOCHS, RNN_BATCH_SIZE)"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":2.167867,"end_time":"2020-11-11T04:09:49.125179","exception":false,"start_time":"2020-11-11T04:09:46.957312","status":"completed"},"tags":[]},"source":["## TabNet\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2020-11-11T04:09:52.515572Z","iopub.status.busy":"2020-11-11T04:09:52.499696Z","iopub.status.idle":"2020-11-11T04:09:52.627042Z","shell.execute_reply":"2020-11-11T04:09:52.626459Z"},"id":"UChxlOR4ZnNo","papermill":{"duration":1.778132,"end_time":"2020-11-11T04:09:52.627166","exception":false,"start_time":"2020-11-11T04:09:50.849034","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["\n","import torch\n","import numpy as np\n","from scipy.sparse import csc_matrix\n","import time\n","from abc import abstractmethod\n","from pytorch_tabnet import tab_network\n","from pytorch_tabnet.multiclass_utils import unique_labels\n","from sklearn.metrics import roc_auc_score, mean_squared_error, accuracy_score\n","from torch.nn.utils import clip_grad_norm_\n","from pytorch_tabnet.utils import (PredictDataset,\n","                                  create_dataloaders,\n","                                  create_explain_matrix)\n","from sklearn.base import BaseEstimator\n","from torch.utils.data import DataLoader\n","from copy import deepcopy\n","import io\n","import json\n","from pathlib import Path\n","import shutil\n","import zipfile\n","\n","class TabModel(BaseEstimator):\n","    def __init__(self, n_d=8, n_a=8, n_steps=3, gamma=1.3, cat_idxs=[], cat_dims=[], cat_emb_dim=1,\n","                 n_independent=2, n_shared=2, epsilon=1e-15,  momentum=0.02,\n","                 lambda_sparse=1e-3, seed=0,\n","                 clip_value=1, verbose=1,\n","                 optimizer_fn=torch.optim.Adam,\n","                 optimizer_params=dict(lr=2e-2),\n","                 scheduler_params=None, scheduler_fn=None,\n","                 mask_type=\"sparsemax\",\n","                 input_dim=None, output_dim=None,\n","                 device_name='auto'):\n","        \n","\n","        self.n_d = n_d\n","        self.n_a = n_a\n","        self.n_steps = n_steps\n","        self.gamma = gamma\n","        self.cat_idxs = cat_idxs\n","        self.cat_dims = cat_dims\n","        self.cat_emb_dim = cat_emb_dim\n","        self.n_independent = n_independent\n","        self.n_shared = n_shared\n","        self.epsilon = epsilon\n","        self.momentum = momentum\n","        self.lambda_sparse = lambda_sparse\n","        self.clip_value = clip_value\n","        self.verbose = verbose\n","        self.optimizer_fn = optimizer_fn\n","        self.optimizer_params = optimizer_params\n","        self.device_name = device_name\n","        self.scheduler_params = scheduler_params\n","        self.scheduler_fn = scheduler_fn\n","        self.mask_type = mask_type\n","        self.input_dim = input_dim\n","        self.output_dim = output_dim\n","\n","        self.batch_size = 1024\n","\n","        self.seed = seed\n","        torch.manual_seed(self.seed)\n","        # Defining device\n","        if device_name == 'auto':\n","            if torch.cuda.is_available():\n","                device_name = 'cuda'\n","            else:\n","                device_name = 'cpu'\n","        self.device = torch.device(device_name)\n","        print(f\"Device used : {self.device}\")\n","\n","    @abstractmethod\n","    def construct_loaders(self, X_train, y_train, X_valid, y_valid,\n","                          weights, batch_size, num_workers, drop_last):\n","        \n","        \n","        raise NotImplementedError('users must define construct_loaders to use this base class')\n","\n","    def init_network(\n","                     self,\n","                     input_dim,\n","                     output_dim,\n","                     n_d,\n","                     n_a,\n","                     n_steps,\n","                     gamma,\n","                     cat_idxs,\n","                     cat_dims,\n","                     cat_emb_dim,\n","                     n_independent,\n","                     n_shared,\n","                     epsilon,\n","                     virtual_batch_size,\n","                     momentum,\n","                     device_name,\n","                     mask_type,\n","                     ):\n","        self.network = tab_network.TabNet(\n","            input_dim,\n","            output_dim,\n","            n_d=n_d,\n","            n_a=n_a,\n","            n_steps=n_steps,\n","            gamma=gamma,\n","            cat_idxs=cat_idxs,\n","            cat_dims=cat_dims,\n","            cat_emb_dim=cat_emb_dim,\n","            n_independent=n_independent,\n","            n_shared=n_shared,\n","            epsilon=epsilon,\n","            virtual_batch_size=virtual_batch_size,\n","            momentum=momentum,\n","            device_name=device_name,\n","            mask_type=mask_type).to(self.device)\n","\n","        self.reducing_matrix = create_explain_matrix(\n","            self.network.input_dim,\n","            self.network.cat_emb_dim,\n","            self.network.cat_idxs,\n","            self.network.post_embed_dim)\n","\n","    def fit(self, X_train, y_train, X_valid=None, y_valid=None, loss_fn=None,\n","            weights=0, max_epochs=100, patience=10, batch_size=1024,\n","            virtual_batch_size=128, num_workers=0, drop_last=False):\n","       \n","               \n","\n","        self.update_fit_params(X_train, y_train, X_valid, y_valid, loss_fn,\n","                               weights, max_epochs, patience, batch_size,\n","                               virtual_batch_size, num_workers, drop_last)\n","\n","        train_dataloader, valid_dataloader = self.construct_loaders(X_train,\n","                                                                    y_train,\n","                                                                    X_valid,\n","                                                                    y_valid,\n","                                                                    self.updated_weights,\n","                                                                    self.batch_size,\n","                                                                    self.num_workers,\n","                                                                    self.drop_last)\n","\n","        self.init_network(\n","            input_dim=self.input_dim,\n","            output_dim=self.output_dim,\n","            n_d=self.n_d,\n","            n_a=self.n_a,\n","            n_steps=self.n_steps,\n","            gamma=self.gamma,\n","            cat_idxs=self.cat_idxs,\n","            cat_dims=self.cat_dims,\n","            cat_emb_dim=self.cat_emb_dim,\n","            n_independent=self.n_independent,\n","            n_shared=self.n_shared,\n","            epsilon=self.epsilon,\n","            virtual_batch_size=self.virtual_batch_size,\n","            momentum=self.momentum,\n","            device_name=self.device_name,\n","            mask_type=self.mask_type\n","        )\n","\n","        self.optimizer = self.optimizer_fn(self.network.parameters(),\n","                                           **self.optimizer_params)\n","\n","        if self.scheduler_fn:\n","            self.scheduler = self.scheduler_fn(self.optimizer, **self.scheduler_params)\n","        else:\n","            self.scheduler = None\n","\n","        self.losses_train = []\n","        self.losses_valid = []\n","        self.learning_rates = []\n","        self.metrics_train = []\n","        self.metrics_valid = []\n","\n","        if self.verbose > 0:\n","            print(\"Will train until validation stopping metric\",\n","                  f\"hasn't improved in {self.patience} rounds.\")\n","            msg_epoch = f'| EPOCH |  train  |   valid  | total time (s)'\n","            print('---------------------------------------')\n","            print(msg_epoch)\n","\n","        total_time = 0\n","        while (self.epoch < self.max_epochs and\n","               self.patience_counter < self.patience):\n","            starting_time = time.time()\n","            # updates learning rate history\n","            self.learning_rates.append(self.optimizer.param_groups[-1][\"lr\"])\n","\n","            fit_metrics = self.fit_epoch(train_dataloader, valid_dataloader)\n","\n","            # leaving it here, may be used for callbacks later\n","            self.losses_train.append(fit_metrics['train']['loss_avg'])\n","            self.losses_valid.append(fit_metrics['valid']['total_loss'])\n","            self.metrics_train.append(fit_metrics['train']['stopping_loss'])\n","            self.metrics_valid.append(fit_metrics['valid']['stopping_loss'])\n","\n","            stopping_loss = fit_metrics['valid']['stopping_loss']\n","            if stopping_loss < self.best_cost:\n","                self.best_cost = stopping_loss\n","                self.patience_counter = 0\n","                # Saving model\n","                self.best_network = deepcopy(self.network)\n","                has_improved = True\n","            else:\n","                self.patience_counter += 1\n","                has_improved=False\n","            self.epoch += 1\n","            total_time += time.time() - starting_time\n","            if self.verbose > 0:\n","                if self.epoch % self.verbose == 0:\n","                    separator = \"|\"\n","                    msg_epoch = f\"| {self.epoch:<5} | \"\n","                    msg_epoch += f\" {fit_metrics['train']['stopping_loss']:.5f}\"\n","                    msg_epoch += f' {separator:<2} '\n","                    msg_epoch += f\" {fit_metrics['valid']['stopping_loss']:.5f}\"\n","                    msg_epoch += f' {separator:<2} '\n","                    msg_epoch += f\" {np.round(total_time, 1):<10}\"\n","                    msg_epoch += f\" {has_improved}\"\n","                    print(msg_epoch)\n","\n","        if self.verbose > 0:\n","            if self.patience_counter == self.patience:\n","                print(f\"Early stopping occured at epoch {self.epoch}\")\n","            print(f\"Training done in {total_time:.3f} seconds.\")\n","            print('---------------------------------------')\n","\n","        self.history = {\"train\": {\"loss\": self.losses_train,\n","                                  \"metric\": self.metrics_train,\n","                                  \"lr\": self.learning_rates},\n","                        \"valid\": {\"loss\": self.losses_valid,\n","                                  \"metric\": self.metrics_valid}}\n","        \n","        self.load_best_model()\n","\n","      \n","        self._compute_feature_importances(train_dataloader)\n","\n","    def save_model(self, path):\n","        \n","        saved_params = {}\n","        for key, val in self.get_params().items():\n","            if isinstance(val, type):\n","               \n","                continue\n","            else:\n","                saved_params[key] = val\n","\n","      \n","        Path(path).mkdir(parents=True, exist_ok=True)\n","\n","        with open(Path(path).joinpath(\"model_params.json\"), \"w\", encoding=\"utf8\") as f:\n","            json.dump(saved_params, f)\n","\n","        \n","        torch.save(self.network.state_dict(), Path(path).joinpath(\"network.pt\"))\n","        shutil.make_archive(path, 'zip', path)\n","        shutil.rmtree(path)\n","        print(f\"Successfully saved model at {path}.zip\")\n","        return f\"{path}.zip\"\n","\n","    def load_model(self, filepath):\n","\n","        try:\n","            with zipfile.ZipFile(filepath) as z:\n","                with z.open(\"model_params.json\") as f:\n","                    loaded_params = json.load(f)\n","                with z.open(\"network.pt\") as f:\n","                    try:\n","                        saved_state_dict = torch.load(f)\n","                    except io.UnsupportedOperation:\n","                       \n","                        saved_state_dict = torch.load(io.BytesIO(f.read()))\n","        except KeyError:\n","            raise KeyError(\"Your zip file is missing at least one component\")\n","\n","        self.__init__(**loaded_params)\n","\n","        self.init_network(\n","            input_dim=self.input_dim,\n","            output_dim=self.output_dim,\n","            n_d=self.n_d,\n","            n_a=self.n_a,\n","            n_steps=self.n_steps,\n","            gamma=self.gamma,\n","            cat_idxs=self.cat_idxs,\n","            cat_dims=self.cat_dims,\n","            cat_emb_dim=self.cat_emb_dim,\n","            n_independent=self.n_independent,\n","            n_shared=self.n_shared,\n","            epsilon=self.epsilon,\n","            virtual_batch_size=1024,\n","            momentum=self.momentum,\n","            device_name=self.device_name,\n","            mask_type=self.mask_type\n","        )\n","        self.network.load_state_dict(saved_state_dict)\n","        self.network.eval()\n","        return\n","\n","    def fit_epoch(self, train_dataloader, valid_dataloader):\n","        \n","        train_metrics = self.train_epoch(train_dataloader)\n","        valid_metrics = self.predict_epoch(valid_dataloader)\n","\n","        fit_metrics = {'train': train_metrics,\n","                       'valid': valid_metrics}\n","\n","        return fit_metrics\n","\n","    @abstractmethod\n","    def train_epoch(self, train_loader):\n","        \n","        raise NotImplementedError('users must define train_epoch to use this base class')\n","\n","    @abstractmethod\n","    def train_batch(self, data, targets):\n","        \n","        raise NotImplementedError('users must define train_batch to use this base class')\n","\n","    @abstractmethod\n","    def predict_epoch(self, loader):\n","        \n","        raise NotImplementedError('users must define predict_epoch to use this base class')\n","\n","    @abstractmethod\n","    def predict_batch(self, data, targets):\n","        \n","        raise NotImplementedError('users must define predict_batch to use this base class')\n","\n","    def load_best_model(self):\n","        if self.best_network is not None:\n","            self.network = self.best_network\n","\n","    @abstractmethod\n","    def predict(self, X):\n","       \n","        raise NotImplementedError('users must define predict to use this base class')\n","\n","    def explain(self, X):\n","        \n","        self.network.eval()\n","\n","        dataloader = DataLoader(PredictDataset(X),\n","                                batch_size=self.batch_size, shuffle=False)\n","\n","        for batch_nb, data in enumerate(dataloader):\n","            data = data.to(self.device).float()\n","\n","            M_explain, masks = self.network.forward_masks(data)\n","            for key, value in masks.items():\n","                masks[key] = csc_matrix.dot(value.cpu().detach().numpy(),\n","                                            self.reducing_matrix)\n","\n","            if batch_nb == 0:\n","                res_explain = csc_matrix.dot(M_explain.cpu().detach().numpy(),\n","                                             self.reducing_matrix)\n","                res_masks = masks\n","            else:\n","                res_explain = np.vstack([res_explain,\n","                                         csc_matrix.dot(M_explain.cpu().detach().numpy(),\n","                                                        self.reducing_matrix)])\n","                for key, value in masks.items():\n","                    res_masks[key] = np.vstack([res_masks[key], value])\n","        return res_explain, res_masks\n","\n","    def _compute_feature_importances(self, loader):\n","        self.network.eval()\n","        feature_importances_ = np.zeros((self.network.post_embed_dim))\n","        for data, targets in loader:\n","            data = data.to(self.device).float()\n","            M_explain, masks = self.network.forward_masks(data)\n","            feature_importances_ += M_explain.sum(dim=0).cpu().detach().numpy()\n","\n","        feature_importances_ = csc_matrix.dot(feature_importances_,\n","                                              self.reducing_matrix)\n","        self.feature_importances_ = feature_importances_ / np.sum(feature_importances_)\n","        \n","        \n","class TabNetRegressor(TabModel):\n","\n","    def construct_loaders(self, X_train, y_train, X_valid, y_valid, weights,\n","                          batch_size, num_workers, drop_last):\n","        \n","        if isinstance(weights, int):\n","            if weights == 1:\n","                raise ValueError(\"Please provide a list of weights for regression.\")\n","        if isinstance(weights, dict):\n","            raise ValueError(\"Please provide a list of weights for regression.\")\n","\n","        train_dataloader, valid_dataloader = create_dataloaders(X_train,\n","                                                                y_train,\n","                                                                X_valid,\n","                                                                y_valid,\n","                                                                weights,\n","                                                                batch_size,\n","                                                                num_workers,\n","                                                                drop_last)\n","        return train_dataloader, valid_dataloader\n","\n","    def update_fit_params(self, X_train, y_train, X_valid, y_valid, loss_fn,\n","                          weights, max_epochs, patience,\n","                          batch_size, virtual_batch_size, num_workers, drop_last):\n","\n","        if loss_fn is None:\n","            self.loss_fn = torch.nn.functional.mse_loss\n","        else:\n","            self.loss_fn = loss_fn\n","\n","        assert X_train.shape[1] == X_valid.shape[1], \"Dimension mismatch X_train X_valid\"\n","        self.input_dim = X_train.shape[1]\n","\n","        if len(y_train.shape) == 1:\n","            raise ValueError(\"\"\"Please apply reshape(-1, 1) to your targets\n","                                if doing single regression.\"\"\")\n","        assert y_train.shape[1] == y_valid.shape[1], \"Dimension mismatch y_train y_valid\"\n","        self.output_dim = y_train.shape[1]\n","\n","        self.updated_weights = weights\n","\n","        self.max_epochs = max_epochs\n","        self.patience = patience\n","        self.batch_size = batch_size\n","        self.virtual_batch_size = virtual_batch_size\n","       \n","        self.patience_counter = 0\n","        self.epoch = 0\n","        self.best_cost = np.inf\n","        self.num_workers = num_workers\n","        self.drop_last = drop_last\n","\n","    def train_epoch(self, train_loader):\n","        \n","\n","        self.network.train()\n","        y_preds = []\n","        ys = []\n","        total_loss = 0\n","\n","        for data, targets in train_loader:\n","            batch_outs = self.train_batch(data, targets)\n","            y_preds.append(batch_outs[\"y_preds\"].cpu().detach().numpy())\n","            ys.append(batch_outs[\"y\"].cpu().detach().numpy())\n","            total_loss += batch_outs[\"loss\"]\n","\n","        y_preds = np.vstack(y_preds)\n","        ys = np.vstack(ys)\n","\n","        \n","        stopping_loss = self.log_loss_multi(ys, torch.sigmoid(torch.as_tensor(y_preds)).numpy()  )\n","        total_loss = total_loss / len(train_loader)\n","        epoch_metrics = {'loss_avg': total_loss,\n","                         'stopping_loss': total_loss,\n","                         }\n","\n","        if self.scheduler is not None:\n","            self.scheduler.step()\n","        return epoch_metrics\n","\n","    def train_batch(self, data, targets):\n","        \n","        self.network.train()\n","        data = data.to(self.device).float()\n","\n","        targets = targets.to(self.device).float()\n","        self.optimizer.zero_grad()\n","\n","        output, M_loss = self.network(data)\n","\n","        y_smo = targets.float() * (1 - LABEL_SMOOTHING) + 0.5 * LABEL_SMOOTHING\n","        loss = self.loss_fn(output, y_smo)\n","        \n","        loss -= self.lambda_sparse*M_loss\n","\n","        loss.backward()\n","        if self.clip_value:\n","            clip_grad_norm_(self.network.parameters(), self.clip_value)\n","        self.optimizer.step()\n","\n","        loss_value = loss.item()\n","        batch_outs = {'loss': loss_value,\n","                      'y_preds': output,\n","                      'y': targets}\n","        return batch_outs\n","\n","\n","    def log_loss_multi(self, y_true, y_pred):\n","        M = y_true.shape[1]\n","        results = np.zeros(M)\n","        for i in range(M):\n","            results[i] = self.log_loss_score(y_true[:,i], y_pred[:,i])\n","        return results.mean()\n","\n","    def log_loss_score(self, actual, predicted,  eps=1e-15):\n","\n","           \n","\n","            \n","            p1 = actual * np.log(predicted+eps)\n","            p0 = (1-actual) * np.log(1-predicted+eps)\n","            loss = p0 + p1\n","\n","            return -loss.mean()\n","\n","    def predict_epoch(self, loader):\n","        \n","        y_preds = []\n","        ys = []\n","        self.network.eval()\n","        total_loss = 0\n","\n","        for data, targets in loader:\n","            batch_outs = self.predict_batch(data, targets)\n","            total_loss += batch_outs[\"loss\"]\n","            y_preds.append(batch_outs[\"y_preds\"].cpu().detach().numpy())\n","            ys.append(batch_outs[\"y\"].cpu().detach().numpy())\n","\n","        y_preds = np.vstack(y_preds)\n","        ys = np.vstack(ys)\n","\n","        stopping_loss = self.log_loss_multi(ys, torch.sigmoid(torch.as_tensor(y_preds)).numpy()  ) #mean_squared_error(y_true=ys, y_pred=y_preds)\n","\n","        total_loss = total_loss / len(loader)\n","        epoch_metrics = {'total_loss': total_loss,\n","                         'stopping_loss': stopping_loss}\n","\n","        return epoch_metrics\n","\n","    def predict_batch(self, data, targets):\n","        \n","        self.network.eval()\n","        data = data.to(self.device).float()\n","        targets = targets.to(self.device).float()\n","\n","        output, M_loss = self.network(data)\n","       \n","        y_smo = targets.float() * (1 - LABEL_SMOOTHING) + 0.5 * LABEL_SMOOTHING\n","        loss = self.loss_fn(output, targets)\n","        \n","        loss -= self.lambda_sparse*M_loss\n","        \n","        loss_value = loss.item()\n","        batch_outs = {'loss': loss_value,\n","                      'y_preds': output,\n","                      'y': targets}\n","        return batch_outs\n","\n","    def predict(self, X):\n","        \n","        self.network.eval()\n","        dataloader = DataLoader(PredictDataset(X),\n","                                batch_size=self.batch_size, shuffle=False)\n","\n","        results = []\n","        for batch_nb, data in enumerate(dataloader):\n","            data = data.to(self.device).float()\n","\n","            output, M_loss = self.network(data)\n","            predictions = output.cpu().detach().numpy()\n","            results.append(predictions)\n","        res = np.vstack(results)\n","        return res "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2020-11-11T04:09:55.899744Z","iopub.status.busy":"2020-11-11T04:09:55.898624Z","iopub.status.idle":"2020-11-11T04:09:55.901299Z","shell.execute_reply":"2020-11-11T04:09:55.901802Z"},"id":"5bY1ib8QZnNq","papermill":{"duration":1.61509,"end_time":"2020-11-11T04:09:55.901969","exception":false,"start_time":"2020-11-11T04:09:54.286879","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["import time\n","import torch\n","\n","def train_and_predict(name, _X_i_train, _y_i_train, _X_i_test, seeds, splits, epochs, batch_size, shuffle_rows=False, pick_col_size=800):\n","    st = time.time()\n","\n","    val_result = train_targets_scored.iloc[:,1:].copy()\n","    val_result.loc[:, :] = 0\n","\n","    sub_result = sample_submission.copy()\n","    sub_result.loc[:, 1:] = 0\n","\n","    all_cols = _X_i_train.columns.values\n","\n","    for h, seed in enumerate(seeds):\n","\n","        seed_everything(seed)\n","\n","        for i, (train_idx, valid_idx) in enumerate(MultilabelStratifiedKFold(n_splits=splits, random_state=seed, shuffle=True)\n","                                      .split(_y_i_train, _y_i_train)):\n","            print(f'Fold {i+1}')\n","\n","            if shuffle_rows:\n","                seed_everything(seed + i)\n","                target_cols = np.random.choice(all_cols, size=pick_col_size)\n","                seed_everything(seed)\n","            else:\n","                target_cols = all_cols\n","\n","            _X_train = torch.as_tensor(_X_i_train.loc[:,target_cols].values[train_idx])\n","            _y_train = torch.as_tensor(_y_i_train.values[train_idx])\n","            _X_valid = torch.as_tensor(_X_i_train.loc[:,target_cols].values[valid_idx])\n","            _y_valid = torch.as_tensor(_y_i_train.values[valid_idx])\n","\n","           \n","            model = TabNetRegressor(n_d=24, n_a=24, n_steps=1, gamma=1.3, lambda_sparse=0, \n","                                    \n","                                    optimizer_fn=torch.optim.Adam,\n","                                    optimizer_params=dict(lr=2e-2, weight_decay=1e-5), mask_type='entmax', \n","                                    device_name=\"cuda\" if torch.cuda.is_available() else \"cpu\", \n","                                    scheduler_params=dict(milestones=[100,150], gamma=0.9), scheduler_fn=torch.optim.lr_scheduler.MultiStepLR)\n","            \n","            model.fit(X_train=_X_train, y_train=_y_train, X_valid=_X_valid, y_valid=_y_valid, max_epochs=epochs, patience=20, batch_size=1024, virtual_batch_size=batch_size,\n","                      num_workers=0, drop_last=False, loss_fn=torch.nn.functional.binary_cross_entropy_with_logits)\n","            model.load_best_model()\n","            \n","            preds = model.predict(_X_valid)\n","            preds = torch.sigmoid(torch.as_tensor(preds)).detach().cpu().numpy()\n","            val_result.iloc[y_train.iloc[valid_idx,:].index.values, :] += preds\n","\n","            preds = model.predict(torch.as_tensor(_X_i_test.loc[test_trt_index, target_cols].values))\n","            preds = torch.sigmoid(torch.as_tensor(preds)).detach().cpu().numpy()\n","            sub_result.loc[test_trt_index, sub_result.columns[1:]] += preds\n","\n","            print('')\n","\n","        tmp_result = val_result.copy()\n","        tmp_result.iloc[:,1:] = val_result.iloc[:,1:] / (h + 1)\n","        print(f' ---- seed:{seed}, ensemble:{h + 1}')\n","        _ = show_metrics(tmp_result, show_each=False)\n","\n","    val_result.iloc[:,1:] = val_result.iloc[:,1:] / len(seeds)\n","    metrics = show_metrics(val_result)\n","\n","    sub_result.iloc[:, 1:] = sub_result.iloc[:, 1:] / (len(seeds) * splits)\n","\n","    print(f' elapsed: {time.time() - st}')\n","\n","    return sub_result, val_result, metrics"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2020-11-11T04:09:59.890547Z","iopub.status.busy":"2020-11-11T04:09:59.889013Z","iopub.status.idle":"2020-11-11T04:29:11.961013Z","shell.execute_reply":"2020-11-11T04:29:11.959287Z"},"id":"tvOxpDxDZnNs","outputId":"d1bf9563-8250-491b-952c-9eeac8ca1d3f","papermill":{"duration":1154.408427,"end_time":"2020-11-11T04:29:11.961147","exception":false,"start_time":"2020-11-11T04:09:57.55272","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["if RUN_TABNET:\n","    tab_result, tab_valid_preds, tab_metrics = train_and_predict('TAB', X_train, y_train, X_test, \n","                                                TAB_SEEDS, TAB_SPLITS, TAB_EPOCHS, TAB_BATCH_SIZE)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2020-11-11T04:29:16.199542Z","iopub.status.busy":"2020-11-11T04:29:16.197691Z","iopub.status.idle":"2020-11-11T04:29:16.200238Z","shell.execute_reply":"2020-11-11T04:29:16.20074Z"},"id":"e8mX9N-kZnNu","papermill":{"duration":1.979713,"end_time":"2020-11-11T04:29:16.200868","exception":false,"start_time":"2020-11-11T04:29:14.221155","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["if not RE_RUN_NN:\n","    ens_result.to_csv('submission.csv', index=False)"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":1.978907,"end_time":"2020-11-11T04:29:20.127321","exception":false,"start_time":"2020-11-11T04:29:18.148414","status":"completed"},"tags":[]},"source":["## Stacking\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2020-11-11T04:29:24.017523Z","iopub.status.busy":"2020-11-11T04:29:23.992888Z","iopub.status.idle":"2020-11-11T04:29:24.036369Z","shell.execute_reply":"2020-11-11T04:29:24.035579Z"},"id":"jf_zrGl_ZnNz","papermill":{"duration":2.02618,"end_time":"2020-11-11T04:29:24.036522","exception":false,"start_time":"2020-11-11T04:29:22.010342","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["import time\n","\n","def re_train_and_predict(name, model_func, _X_i_train, _y_i_train, _X_i_test, seeds, splits, epochs, batch_size, shuffle_rows=False, pick_col_size=800):\n","  st = time.time()\n","\n","  val_result = train_targets_scored.loc[:,_y_i_train.columns].copy()\n","  val_result.loc[:, :] = 0\n","\n","  sub_result = sample_submission.loc[:, _y_i_train.columns].copy()\n","\n","  sub_result.loc[:, :] = 0\n","\n","  all_cols = _X_i_train.columns.values\n","\n","\n","  for h, seed in enumerate(seeds):\n","\n","      seed_everything(seed)\n","\n","      for i, (train_idx, valid_idx) in enumerate(MultilabelStratifiedKFold(n_splits=splits, random_state=seed, shuffle=True)\n","                                      .split(_y_i_train, _y_i_train)):\n","          print(f'Fold {i+1}')\n","\n","          if shuffle_rows:\n","            seed_everything(seed + i)\n","            target_cols = np.random.choice(all_cols, size=pick_col_size)\n","            seed_everything(seed)\n","          else:\n","            target_cols = all_cols\n","\n","          _X_train = _X_i_train.loc[:,target_cols].values[train_idx]\n","          _y_train = _y_i_train.values[train_idx]\n","          _X_valid = _X_i_train.loc[:,target_cols].values[valid_idx]\n","          _y_valid = _y_i_train.values[valid_idx]\n","\n","          model = model_func(len(target_cols), len(_y_i_train.columns))\n","          \n","          model.fit(_X_train, _y_train,\n","                  validation_data=(_X_valid, _y_valid),\n","                  epochs=epochs, batch_size=batch_size,\n","                  callbacks=[\n","                      ReduceLROnPlateau(monitor='val_logloss', factor=0.1, patience=3, verbose=1, min_delta=1e-4, mode='min')\n","                      , ModelCheckpoint(f'{name}_{seed}_{i}.hdf5', monitor = 'val_logloss', verbose = 0, save_best_only = True, save_weights_only = True, mode = 'min')\n","                      , EarlyStopping(monitor = 'val_logloss', min_delta = 1e-4, patience = 5, mode = 'min', baseline = None, restore_best_weights = True)\n","                  ], verbose=2)\n","      \n","          model.load_weights(f'{name}_{seed}_{i}.hdf5')\n","          val_result.iloc[y_train.iloc[valid_idx,:].index.values, :] += model.predict(_X_valid)\n","          sub_result.loc[test_trt_index, :] += model.predict(_X_i_test.loc[test_trt_index, target_cols])\n","\n","          print('')\n","\n","      tmp_result = val_result.copy()\n","      tmp_result.iloc[:,1:] = val_result.iloc[:,1:] / (h + 1)\n","      print(f' ---- seed:{seed}, ensemble:{h + 1}')\n","      _ = show_metrics(tmp_result, show_each=False)\n","\n","  val_result.iloc[:,1:] = val_result.iloc[:,1:] / len(seeds)\n","  metrics = show_metrics(val_result)\n","\n","  sub_result.iloc[:, :] = sub_result.iloc[:, :] / (len(seeds) * splits)\n","\n","  print(f' elapsed: {time.time() - st}')\n","\n","  return sub_result, val_result, metrics"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2020-11-11T04:29:28.041526Z","iopub.status.busy":"2020-11-11T04:29:28.039892Z","iopub.status.idle":"2020-11-11T04:29:28.799606Z","shell.execute_reply":"2020-11-11T04:29:28.798591Z"},"id":"8zOxkk6eZnN0","papermill":{"duration":2.68108,"end_time":"2020-11-11T04:29:28.799731","exception":false,"start_time":"2020-11-11T04:29:26.118651","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["X_stacking_train = pd.concat([nn_valid_preds, rnn_valid_preds], axis=1, ignore_index=True)\n","X_stacking_train = pd.concat([X_stacking_train, tab_valid_preds], axis=1, ignore_index=True)\n","X_stacking_train = pd.concat([X_stacking_train, nn2_valid_preds], axis=1, ignore_index=True)\n","X_stacking_train = pd.concat([X_stacking_train, snn_valid_preds], axis=1, ignore_index=True)\n","X_stacking_train = X_stacking_train[train_trt_index]\n","\n","X_stacking_test = pd.concat([nn_result.iloc[:, 1:], rnn_result.iloc[:, 1:]], axis=1, ignore_index=True)\n","X_stacking_test = pd.concat([X_stacking_test, tab_result.iloc[:, 1:]], axis=1, ignore_index=True)\n","X_stacking_test = pd.concat([X_stacking_test, nn2_result.iloc[:, 1:]], axis=1, ignore_index=True)\n","X_stacking_test = pd.concat([X_stacking_test, snn_result.iloc[:, 1:]], axis=1, ignore_index=True)\n","X_stacking_test = X_stacking_test"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2020-11-11T04:29:32.605564Z","iopub.status.busy":"2020-11-11T04:29:32.601758Z","iopub.status.idle":"2020-11-11T04:29:32.609621Z","shell.execute_reply":"2020-11-11T04:29:32.608825Z"},"id":"c2fnorDfZnN2","papermill":{"duration":1.911485,"end_time":"2020-11-11T04:29:32.609736","exception":false,"start_time":"2020-11-11T04:29:30.698251","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["def create_stacking_model(input_dim, output_dim):\n","    print(f'the input dim is {input_dim}')\n","\n","    model = M.Sequential()\n","    model.add(L.Input(input_dim))\n","    model.add(L.BatchNormalization())\n","    model.add(L.Dropout(0.3))\n","    model.add(WeightNormalization(L.Dense(input_dim, activation='elu')))\n","\n","    model.add(L.BatchNormalization())\n","    model.add(L.Dropout(0.5))\n","    model.add(WeightNormalization(L.Dense(input_dim, activation='swish')))\n","\n","    model.add(L.BatchNormalization())\n","    model.add(L.Dropout(0.5))\n","    model.add(WeightNormalization(L.Dense(output_dim, activation='sigmoid', bias_initializer=output_bias)))\n","\n","    model.compile(optimizer=tfa.optimizers.Lookahead(tf.optimizers.Adam(lr=1e-3), sync_period=10),\n","                    loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=LABEL_SMOOTHING), metrics=logloss)\n","\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2020-11-11T04:29:36.747893Z","iopub.status.busy":"2020-11-11T04:29:36.746547Z","iopub.status.idle":"2020-11-11T04:40:00.600797Z","shell.execute_reply":"2020-11-11T04:40:00.60131Z"},"id":"_BkmvXigZnN3","outputId":"39b8b19d-577d-4ed7-d8b6-054de86c30aa","papermill":{"duration":626.10266,"end_time":"2020-11-11T04:40:00.601462","exception":false,"start_time":"2020-11-11T04:29:34.498802","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["if RUN_STACKING:\n","    st_re_result, st_re_valid_preds, st_re_metrics = re_train_and_predict('ST_NN', create_stacking_model, \n","                                                                         X_stacking_train, y_train, X_stacking_test, \n","                                                                         STK_SEEDS, STK_SPLITS, STK_EPOCHS, STK_BATCH_SIZE)"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":2.566378,"end_time":"2020-11-11T04:40:05.237503","exception":false,"start_time":"2020-11-11T04:40:02.671125","status":"completed"},"tags":[]},"source":["## Blending\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","import pickle\n","with open('nn_valid_preds.pkl', 'wb') as p:\n","  pickle.dump(nn_valid_preds , p)\n","with open('nn2_valid_preds.pkl', 'wb') as p:\n","  pickle.dump(nn2_valid_preds , p)\n","with open('rnn_valid_preds.pkl', 'wb') as p:\n","  pickle.dump(rnn_valid_preds , p)\n","with open('tab_valid_preds.pkl', 'wb') as p:\n","  pickle.dump(tab_valid_preds , p)\n","with open('st_re_valid_preds.pkl', 'wb') as p:\n","  pickle.dump(st_re_valid_preds , p)\n","with open('snn_valid_preds.pkl', 'wb') as p:\n","  pickle.dump(snn_valid_preds , p)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2020-11-11T04:40:09.419855Z","iopub.status.busy":"2020-11-11T04:40:09.418652Z","iopub.status.idle":"2020-11-11T04:40:12.333625Z","shell.execute_reply":"2020-11-11T04:40:12.332616Z"},"id":"3voxj4MMZnN5","outputId":"a2841aeb-a8db-4515-8f55-23be78a43821","papermill":{"duration":4.991924,"end_time":"2020-11-11T04:40:12.333742","exception":false,"start_time":"2020-11-11T04:40:07.341818","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["ratio = [0.05, 0.05, 0.1, 0.25, 0.35, 0.2]\n","\n","ens_valid_preds = nn_valid_preds * ratio[0] + nn2_valid_preds * ratio[1] + rnn_valid_preds * ratio[2] + tab_valid_preds * ratio[3] + \\\n","                  st_re_valid_preds * ratio[4] + snn_valid_preds * ratio[5]\n","_ = show_metrics(ens_valid_preds, show_each=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2020-11-11T04:40:16.686314Z","iopub.status.busy":"2020-11-11T04:40:16.683997Z","iopub.status.idle":"2020-11-11T04:40:19.027969Z","shell.execute_reply":"2020-11-11T04:40:19.026758Z"},"id":"COEEW3d0ZnN7","papermill":{"duration":4.395987,"end_time":"2020-11-11T04:40:19.028102","exception":false,"start_time":"2020-11-11T04:40:14.632115","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["ens_result = nn_result.iloc[:, 1:] * ratio[0] + nn2_result.iloc[:, 1:] * ratio[1] + rnn_result.iloc[:, 1:] * ratio[2] + \\\n","             tab_result.iloc[:, 1:] * ratio[3] + st_re_result * ratio[4] + snn_result.iloc[:, 1:] * ratio[5]\n","ens_result = pd.concat([nn_result.iloc[:,0], ens_result], axis=1)\n","\n","ens_result.to_csv('results.csv', index=False)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"databundleVersionId":1651354,"sourceId":19988,"sourceType":"competition"},{"datasetId":874046,"sourceId":1489068,"sourceType":"datasetVersion"},{"datasetId":866848,"sourceId":2582899,"sourceType":"datasetVersion"}],"dockerImageVersionId":30015,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
